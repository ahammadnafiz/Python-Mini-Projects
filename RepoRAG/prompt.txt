
    The following is the content of a GitHub repository converted into text format. The repository contains various files and directories, each serving a specific purpose. Please analyze and understand the content of this repository.

    Repository Content:
    File: README.md
Size: 4486 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT
Content:
![Banner](assets/banner.png)


<h1 align="center">RepoRAG</h1>

A fully interactive tool designed to streamline your GitHub repository prompt generation process and facilitate RAG (Retrieval-Augmented Generation) workflows


## 🏗️ System Design & Architecture
- Below is the high-level architecture of the system:

![Flow](assets/reporagai_2.jpeg)
![Architecture](assets/reporagai_1.jpeg)


## 📌 Table of Contents
- [🌟 Features](#-features)
- [🛠️ Installation](#️-installation)
- [🚀 Usage](#-usage)
- [⚙️ Configuration](#%EF%B8%8F-configuration)
- [📖 Interactive Commands](#-interactive-commands)
- [🎨 Icons and Badges](#-icons-and-badges)
- [🧪 Example](#-example)
- [🤖 Technologies Used](#-technologies-used)
- [📃 License](#-license)
- [🤝 Contributing](#-contributing)

---

## 🌟 Features
- **Interactive Mode**: Easily input repository details and start generating prompts through a user-friendly interface.
- **RAG Support**: Engage in interactive Retrieval-Augmented Generation sessions with real-time question answering about your repositories.
- **Environment Variable Management**: Supports environment-based configurations for quick and seamless setup.
- **Error Handling**: Robust error handling to ensure smooth operations during interactive sessions.
- **Clear Command Interface**: Offers a set of easy-to-use commands for RAG sessions, including options to clear history or exit the session.

---

## 🛠️ Installation

Follow these steps to get started with RepoRAG:

1. **Clone the repository**:
```bash
$ git clone https://github.com/ahammadnafiz/RepoRAG.git
```

2. **Navigate to the project directory**:
```bash
$ cd RepoRAG
```

3. **Install the required dependencies**:
```bash
$ pip install -r requirements.txt
```

4. **Create a `.env` file** to store your environment variables (see [Configuration](#%EF%B8%8F-configuration)).

---

## 🚀 Usage

Run the interactive mode directly from your terminal:
```bash
$ python reporag.py
```

### 🧑‍💻 Interactive Session Walkthrough:
- **Step 1**: Enter the repository URL in the format `owner/repo`.
- **Step 2**: Provide your GitHub access token.
- **Step 3**: Specify the output file name for the generated prompt.
- **Step 4**: Choose the desired mode:
  - `1`: Generate a prompt file only.
  - `2`: Generate a prompt file and start a RAG session.

---

## ⚙️ Configuration

Create a `.env` file in the root directory to store your environment variables:
```env
GITHUB_ACCESS_TOKEN=your_github_token
GROQ_API_KEY=your_groq_api_key
```

**Environment Variables Explained**:
- `GITHUB_ACCESS_TOKEN`: Your personal GitHub token for accessing private repositories.
- `GROQ_API_KEY`: API key required for advanced RAG functionalities.

---

## 📖 Interactive Commands
When running in RAG mode, you can use the following commands:

| Command | Description                    |
| ------- | ------------------------------ |
| `exit`  | Exit the RAG session           |
| `help`  | Display available commands     |
| `clear` | Clear the conversation history |

---

## 🎨 Icons and Badges

![Python](https://img.shields.io/badge/python-v3.8%2B-blue) ![GitHub](https://img.shields.io/badge/github-RepoRAG-lightgrey)

**Supported Technologies**:
- Python 3.8+
- GitHub API
- dotenv for environment variable management

---

## 🧪 Example

```bash
=== RepoRAG Interactive Mode ===

Enter the GitHub repository URL (format: owner/repo): ahammadnafiz/RepoRAG
GitHub access token not found in .env. Please enter your token: ********
Enter the output file name for the prompt (e.g., output_prompt.txt): my_prompt.txt
Available modes:
1. Generate prompt file only
2. Generate prompt file and start RAG mode
Enter your choice (1/2): 2
```

---

## 🤖 Technologies Used
- **Python**: Core language for the tool.
- **dotenv**: For environment variable management.
- **Typing**: Used for type hints and validation.
- **GitHub API**: To interact with GitHub repositories.

---

## 📃 License

This project is licensed under the MIT License. See the [LICENSE](https://github.com/ahammadnafiz/RepoRAG/blob/main/LICENSE) file for details.

---

## 🤝 Contributing

We welcome contributions to RepoRAG! Please follow these steps:

1. Fork the repository.
2. Create a new branch (`git checkout -b feature/your-feature`).
3. Commit your changes (`git commit -m 'Add your feature'`).
4. Push to the branch (`git push origin feature/your-feature`).
5. Create a pull request.


File: app.py
Size: 9 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT
Content:
# app.py


Directory: assets

File: assets/banner.png (binary or non-UTF-8 content)
Size: 12931 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT

File: assets/reporagai_1.jpeg (binary or non-UTF-8 content)
Size: 135427 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT

File: assets/reporagai_2.jpeg (binary or non-UTF-8 content)
Size: 75306 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT

File: reporag.py
Size: 5855 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT
Content:
from reporag.reporag.main import main
from dotenv import load_dotenv
import os
import sys
from typing import Dict, Any, Optional

def validate_repo_url(url: str) -> bool:
    """Validate the repository URL format."""
    parts = url.split('/')
    return len(parts) == 2 and all(part.strip() for part in parts)

def get_env_or_input(env_var: str, prompt: str) -> str:
    """Get value from environment variable or user input."""
    value = os.getenv(env_var)
    if not value:
        value = input(prompt).strip()
    return value

def get_user_input() -> Dict[str, Any]:
    """Get and validate user input for repository processing."""
    print("\n=== Repoprompter Interactive Mode ===\n")

    # Get and validate repository URL
    while True:
        repo_url = input("Enter the GitHub repository URL (format: owner/repo): ").strip()
        if validate_repo_url(repo_url):
            break
        print("Invalid repository format. Please use 'owner/repo' format.")

    # Get tokens
    github_token = get_env_or_input(
        'GITHUB_ACCESS_TOKEN',
        "GitHub access token not found in .env. Please enter your token: "
    )

    # Get output file name (now required for all modes)
    while True:
        output_file = input("\nEnter the output file name for the prompt (e.g., output_prompt.txt): ").strip()
        if output_file:
            break
        print("Output file name cannot be empty.")

    # Choose mode with input validation
    while True:
        print("\nAvailable modes:")
        print("1. Generate prompt file only")
        print("2. Generate prompt file and start RAG mode")
        mode = input("Enter your choice (1/2): ").strip()
        if mode in ['1', '2']:
            break
        print("Invalid choice. Please enter 1 or 2.")

    result = {
        'repo_url': repo_url,
        'github_token': github_token,
        'output_file': output_file,
        'mode': 'prompt' if mode == '1' else 'rag'
    }

    if mode == '2':
        result['groq_token'] = get_env_or_input(
            'GROQ_API_KEY',
            "Groq API key not found in .env. Please enter your key: "
        )

    return result

def interactive_rag_session(rag_instance: Any) -> None:
    """Run an interactive RAG session with improved error handling."""
    print("\n=== RAG Interactive Mode ===")
    print("Available commands:")
    print("- exit: Exit the RAG session")
    print("- help: Show this help message")
    print("- clear: Clear the conversation history")
    print("- Any other input will be treated as a question about the repository\n")

    while True:
        try:
            question = input("\nQuestion: ").strip()

            if not question:
                print("Please enter a question or command.")
                continue

            command = question.lower()
            if command == 'exit':
                print("Exiting RAG session...")
                rag_instance.clear_vector_store()  # Clear the vector store
                rag_instance.clear_cache()  # Clear the cache
                break
            elif command == 'help':
                print("\nAvailable commands:")
                print("- exit: Exit the RAG session")
                print("- help: Show this help message")
                print("- clear: Clear the conversation history")
                print("- Any other input will be treated as a question about the repository")
                continue
            elif command == 'clear':
                rag_instance.clear_memory()
                print("Conversation history cleared.")
                continue

            print("\nProcessing your question...")
            answer = rag_instance.query(question)
            print("\nAnswer:", answer)

        except KeyboardInterrupt:
            print("\nExiting RAG session...")
            rag_instance.clear_vector_store()  # Clear the vector store
            rag_instance.clear_cache()  # Clear the cache
            break
        except Exception as e:
            print(f"\nError processing question: {str(e)}")
            print("Please try another question or type 'exit' to quit.")

def main_interactive() -> None:
    """Main interactive function with improved error handling."""
    try:
        # Load environment variables
        load_dotenv(override=True)

        # Get user inputs
        inputs = get_user_input()

        print("\nInitializing...")

        if inputs['mode'] == 'prompt':
            # Generate prompt file only
            result = main(
                repo_url=inputs['repo_url'],
                access_token=inputs['github_token'],
                output_file=inputs['output_file']
            )

            if isinstance(result, str) and result.startswith("Error"):
                raise Exception(result)

            print(f"\nPrompt successfully generated and saved to {inputs['output_file']}")

        else:
            # Generate prompt file and initialize RAG
            result = main(
                repo_url=inputs['repo_url'],
                access_token=inputs['github_token'],
                groq_api_key=inputs['groq_token'],
                output_file=inputs['output_file'],
                rag_mode=True
            )

            if isinstance(result, str) and result.startswith("Error"):
                raise Exception(result)

            print(f"\nPrompt file generated and saved to {inputs['output_file']}")
            print("RAG system initialized successfully.")

            # Start interactive RAG session
            interactive_rag_session(result)

    except KeyboardInterrupt:
        print("\nOperation cancelled by user.")
        sys.exit(0)
    except Exception as e:
        print(f"\nError: {str(e)}")
        print("Please check your inputs and try again.")
        sys.exit(1)

if __name__ == "__main__":
    main_interactive()

Directory: reporag

File: reporag/__init__.py
Size: 150 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT
Content:
# repoprompter/__init__.py
"""
RepoRAG: A Python package to convert a GitHub repository to a prompted text format for LLMs.
"""

__version__ = "0.1.0"

Directory: reporag/reporag

File: reporag/reporag/__init__.py
Size: 390 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT
Content:
# repoprompter/repoprompter/__init__.py
from .main import main
from .fetcher import fetch_repo_content
from .converter import convert_to_text
from .structurer import structure_text_for_llm
from .prompter import create_prompt
from .rag import RepoRAG

__all__ = [
    'main',
    'fetch_repo_content',
    'convert_to_text',
    'structure_text_for_llm',
    'create_prompt',
    'RepoRAG'
]

File: reporag/reporag/converter.py
Size: 1071 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT
Content:
# repoprompter/repoprompter/converter.py
def convert_to_text(contents, repo):
    text_content = []
    for content in contents:
        if content.type == "file":
            try:
                if content.encoding is None or content.encoding == 'none':
                    raise UnicodeDecodeError("unsupported encoding", b"", 0, 1, "none")
                file_content = content.decoded_content.decode('utf-8')
                file_info = f"File: {content.path}\nSize: {content.size} bytes\nLast Modified: {content.last_modified}\nContent:\n{file_content}\n"
                text_content.append(file_info)
            except (UnicodeDecodeError, AttributeError):
                text_content.append(f"File: {content.path} (binary or non-UTF-8 content)\nSize: {content.size} bytes\nLast Modified: {content.last_modified}\n")
        elif content.type == "dir":
            dir_info = f"Directory: {content.path}\n"
            text_content.append(dir_info)
            text_content.extend(convert_to_text(repo.get_contents(content.path), repo))
    return text_content

File: reporag/reporag/fetcher.py
Size: 848 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT
Content:
# repoprompter/repoprompter/fetcher.py
from github import Github, GithubException

def fetch_repo_content(repo_url, access_token):
    try:
        g = Github(access_token)
        repo = g.get_repo(repo_url)
        contents = repo.get_contents("")
        # print(f"Repository content fetched: {contents} items")
        return repo, contents
    except GithubException as e:
        if e.status == 404:
            raise Exception(f"Repository not found: {repo_url}. Please check the repository URL and access token.")
        elif e.status == 401:
            raise Exception(f"Unauthorized access: {repo_url}. Please check your access token and permissions.")
        else:
            raise Exception(f"GitHub API error: {e.data['message']}")
    except Exception as e:
        raise Exception(f"Error fetching repository content: {str(e)}")


File: reporag/reporag/main.py
Size: 1484 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT
Content:
# repoprompter/repoprompter/main.py
from .fetcher import fetch_repo_content
from .converter import convert_to_text
from .structurer import structure_text_for_llm
from .prompter import create_prompt
from .rag import RepoRAG

def main(repo_url: str, access_token: str, groq_api_key: str = None,
         output_file: str = None, rag_mode: bool = False):
    """Main function to process repository content with optional RAG support."""
    try:
        # Always fetch and process content first
        repo, contents = fetch_repo_content(repo_url, access_token)
        text_content = convert_to_text(contents, repo)
        structured_text = structure_text_for_llm(text_content)

        # Debug: Print the structured text
        # print("Structured Text:")
        # print(structured_text)

        # Always generate and save the prompt first
        prompt = create_prompt(structured_text)
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as file:
                file.write(prompt)
            print(f"Prompt saved to {output_file}")

        # If RAG mode is enabled, initialize RAG with the processed content
        if rag_mode:
            if not groq_api_key:
                raise ValueError("Groq API key is required for RAG mode")

            rag = RepoRAG(groq_api_key=groq_api_key)
            rag.ingest_content(structured_text)
            return rag

        return prompt

    except Exception as e:
        return f"Error: {str(e)}"


File: reporag/reporag/prompter.py
Size: 857 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT
Content:
# repoprompter/repoprompter/prompter.py
def create_prompt(structured_text):
    prompt = f"""
    The following is the content of a GitHub repository converted into text format. The repository contains various files and directories, each serving a specific purpose. Please analyze and understand the content of this repository.

    Repository Content:
    {structured_text}

    Instructions:
    1. Identify the main purpose of the repository.
    2. Summarize the key functionalities provided by the repository.
    3. Highlight any important files or directories and their roles.
    4. Note any dependencies or requirements specified in the repository.
    5. Provide any additional insights or observations about the repository's structure and content.

    Please provide a detailed analysis based on the above instructions.
    """
    return prompt

File: reporag/reporag/rag.py
Size: 25796 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT
Content:
import logging
from typing import List, Dict, Any, Optional, Tuple
import shutil
import os
import hashlib
import pickle
import re
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_groq import ChatGroq
from langchain.chains import ConversationalRetrievalChain

# Download required NLTK data
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)

@dataclass
class CodeChunk:
    """Represents a chunk of code with metadata."""
    content: str
    file_path: str
    start_line: int
    end_line: int
    language: str

class RepoRAGSplitter:
    """Custom text splitter for repository content."""

    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))

        # Regex patterns for different content types
        self.patterns = {
            'function': r'(?:def|class)\s+[a-zA-Z_][a-zA-Z0-9_]*\s*\([^)]*\):',
            'markdown_section': r'^#{1,6}\s+.+$',
            'code_block': r'```[\s\S]*?```',
            'file_header': r'File:\s+([^\n]+)',
            'import_statement': r'^(?:from|import)\s+\w+',
        }

    def split_content(self, content: str) -> List[Document]:
        """Split content based on content type and semantic boundaries."""
        chunks = []

        # First, try to identify the content type
        content_type = self._identify_content_type(content)

        # Split based on content type
        if content_type == "code":
            raw_chunks = self._split_code(content)
        elif content_type == "markdown":
            raw_chunks = self._split_markdown(content)
        else:
            raw_chunks = self._split_generic(content)

        # Process each chunk and convert to Document
        for chunk in raw_chunks:
            processed_chunk = self._process_chunk(chunk, content_type)
            # Convert dict to Document
            doc = Document(
                page_content=processed_chunk["page_content"],
                metadata=processed_chunk["metadata"]
            )
            chunks.append(doc)

        return chunks

    def _identify_content_type(self, content: str) -> str:
        """Identify the type of content."""
        if re.search(self.patterns['import_statement'], content) or '.py' in content[:100]:
            return "code"
        elif content.startswith('# ') or '```' in content:
            return "markdown"
        return "text"

    def _split_code(self, content: str) -> List[str]:
        """Split code content preserving function/class boundaries."""
        chunks = []
        lines = content.split('\n')
        current_chunk = []
        current_size = 0
        in_function = False

        for i, line in enumerate(lines):
            # Check for function/class definition
            if re.match(self.patterns['function'], line):
                if current_chunk and (not in_function or current_size >= self.chunk_size):
                    chunks.append('\n'.join(current_chunk))
                    current_chunk = []
                    current_size = 0
                in_function = True

            current_chunk.append(line)
            current_size += len(line)

            # Check if we should split
            if not in_function and current_size >= self.chunk_size:
                chunks.append('\n'.join(current_chunk))
                current_chunk = []
                current_size = 0

            # Check if function ends
            if in_function and line.strip() == "" and i < len(lines) - 1:
                if not lines[i + 1].startswith(' '):
                    in_function = False

        if current_chunk:
            chunks.append('\n'.join(current_chunk))

        return chunks

    def _split_markdown(self, content: str) -> List[str]:
        """Split markdown content preserving section boundaries."""
        chunks = []
        current_chunk = []
        current_size = 0

        for line in content.split('\n'):
            # Check for new section
            if re.match(self.patterns['markdown_section'], line):
                if current_chunk and current_size >= self.chunk_size:
                    chunks.append('\n'.join(current_chunk))
                    current_chunk = []
                    current_size = 0

            current_chunk.append(line)
            current_size += len(line)

            # Split if chunk is too large
            if current_size >= self.chunk_size:
                chunks.append('\n'.join(current_chunk))
                current_chunk = []
                current_size = 0

        if current_chunk:
            chunks.append('\n'.join(current_chunk))

        return chunks

    def _split_generic(self, content: str) -> List[str]:
        """Split generic content using sentence boundaries."""
        sentences = sent_tokenize(content)
        chunks = []
        current_chunk = []
        current_size = 0

        for sentence in sentences:
            current_size += len(sentence)

            if current_size >= self.chunk_size:
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                current_chunk = [sentence]
                current_size = len(sentence)
            else:
                current_chunk.append(sentence)

        if current_chunk:
            chunks.append(' '.join(current_chunk))

        return chunks

    def _process_chunk(self, chunk: str, content_type: str) -> Dict[str, Any]:
        """Process chunk and add metadata with proper type handling."""
        # Extract file path if present
        file_path = None
        file_match = re.search(self.patterns['file_header'], chunk)
        if file_match:
            file_path = file_match.group(1)

        # Ensure all metadata values are valid types (str, int, float, or bool)
        metadata = {
            "content_type": content_type or "unknown",
            "file_path": file_path or "unknown",
            "language": self._detect_language(chunk),
            "size": len(chunk),
            "tokens": len(word_tokenize(chunk))
        }

        # Filter out any None values and ensure proper types
        filtered_metadata = {}
        for key, value in metadata.items():
            if value is not None:
                # Convert to appropriate type if needed
                if isinstance(value, (str, int, float, bool)):
                    filtered_metadata[key] = value
                else:
                    # Convert other types to string
                    filtered_metadata[key] = str(value)

        return {
            "page_content": chunk,
            "metadata": filtered_metadata
        }

    def _preprocess_text(self, text: str) -> str:
        """Preprocess text content."""
        # Lowercase
        text = text.lower()

        # Tokenize
        words = word_tokenize(text)

        # Remove stop words and lemmatize
        words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]

        return ' '.join(words)

    def _detect_language(self, content: str) -> str:
        """Detect programming language from content."""
        if '.py' in content or 'def ' in content:
            return 'python'
        elif '.js' in content or 'function ' in content:
            return 'javascript'
        elif '.java' in content or 'class ' in content:
            return 'java'
        return 'unknown'

class RepoRAG:
    """RAG system optimized for repository content."""

    def __init__(
        self,
        groq_api_key: str,
        model_name: str = "mixtral-8x7b-32768",
        embedding_model: str = "BAAI/bge-small-en-v1.5",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        k_retrieval: int = 4,
        cache_dir: str = "./cache"
    ):
        self.setup_logging()
        self.initialize_components(
            groq_api_key, model_name, embedding_model,
            chunk_size, chunk_overlap, k_retrieval, cache_dir
        )

    def setup_logging(self):
        """Set up logging configuration."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def initialize_components(
        self,
        groq_api_key: str,
        model_name: str,
        embedding_model: str,
        chunk_size: int,
        chunk_overlap: int,
        k_retrieval: int,
        cache_dir: str
    ):
        """Initialize RAG components."""
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.k_retrieval = k_retrieval
        self.cache_dir = cache_dir
        self.chat_history = []

        # Initialize custom splitter
        self.splitter = RepoRAGSplitter(chunk_size, chunk_overlap)

        # Initialize embeddings
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # Initialize LLM
        self.llm = ChatGroq(
            api_key=groq_api_key,
            model_name=model_name
        )

        # Create cache directory
        os.makedirs(self.cache_dir, exist_ok=True)

        # Initialize prompt templates
        self._initialize_prompts()

    def _initialize_prompts(self):
        """Initialize different prompt templates for different types of queries."""
        self.prompts = {
            "code": """You are a code analysis assistant. Focus on:
                1. Code structure and patterns
                2. Implementation details
                3. Best practices and potential improvements
                4. Dependencies and requirements

                Current context:
                {context}

                Question: {question}
                """,

            "documentation": """You are a documentation specialist. Focus on:
                1. Project overview and purpose
                2. Installation and setup
                3. Usage instructions
                4. API documentation

                Current context:
                {context}

                Question: {question}
                """,

            "general": """You are a repository analysis assistant. Focus on:
                1. Repository structure and organization
                2. Code and documentation quality
                3. Implementation patterns
                4. Project requirements

                Current context:
                {context}

                Question: {question}
                """
        }

    def _filter_documents_metadata(self, documents: List[Document]) -> List[Document]:
        """Filter document metadata to ensure compatibility with Chroma."""
        filtered_documents = []
        for doc in documents:
            if doc.metadata:
                # Create new filtered metadata dictionary
                filtered_metadata = {}
                for key, value in doc.metadata.items():
                    if value is not None and isinstance(value, (str, int, float, bool)):
                        filtered_metadata[key] = value
                    elif value is not None:
                        # Convert other types to string
                        filtered_metadata[key] = str(value)

                # Create new document with filtered metadata
                filtered_doc = Document(
                    page_content=doc.page_content,
                    metadata=filtered_metadata
                )
                filtered_documents.append(filtered_doc)
            else:
                # If no metadata, add empty dict
                filtered_doc = Document(
                    page_content=doc.page_content,
                    metadata={}
                )
                filtered_documents.append(filtered_doc)

        return filtered_documents

    def ingest_content(self, content: str) -> None:
        """Ingest repository content with specialized processing."""
        try:
            # Clear existing vector store
            self.clear_vector_store()

            # Process content with custom splitter
            documents = self.splitter.split_content(content)

            # Filter metadata before creating vector store
            filtered_documents = self._filter_documents_metadata(documents)

            # Create vector store with filtered documents
            self.vector_store = Chroma.from_documents(
                documents=filtered_documents,
                embedding=self.embeddings,
                persist_directory="./repo_chroma_db"
            )

            # Initialize retrieval chain
            self._initialize_qa_chain()

            self.logger.info("Content ingestion completed successfully")

        except Exception as e:
            self.logger.error(f"Error during content ingestion: {str(e)}")
            raise

    def _initialize_qa_chain(self):
        """Initialize the QA chain with custom prompts and retrieval settings."""
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={
                    "k": self.k_retrieval,
                    "filter": None
                }
            ),
            return_source_documents=True,
            verbose=False
        )

    def query(
        self,
        question: str,
        context_filter: Optional[Dict] = None,
        query_type: str = "general"
    ) -> Dict[str, Any]:
        """Query the RAG system with custom filtering and response generation."""
        if not self.vector_store:
            raise ValueError("No content ingested. Call ingest_content first.")

        try:
            # Generate cache key
            cache_key = self._generate_cache_key(question, context_filter, query_type)

            # Check cache
            cached_response = self._check_cache(cache_key)
            if cached_response:
                return cached_response

            # Update retriever filter
            self._update_retriever_filter(query_type, context_filter)

            # Get appropriate prompt
            prompt = self.prompts.get(query_type, self.prompts["general"])

            # Execute query
            response = self.qa_chain({
                "question": question,
                "chat_history": self.get_chat_history()
            })

            # Process and structure response
            processed_response = self._process_response(response, query_type)

            # Update chat history
            self._update_chat_history(question, processed_response["answer"])

            # Cache response
            self._cache_response(cache_key, processed_response)

            return processed_response

        except Exception as e:
            self.logger.error(f"Error in query: {str(e)}")
            raise

    def _process_response(self, response: Dict[str, Any], query_type: str) -> Dict[str, Any]:
        """Process and structure the response based on query type."""
        # Format the answer for terminal readability
        formatted_answer = self._format_for_terminal(response["answer"])

        processed_response = {
            "answer": formatted_answer,
            "sources": [],
            "metadata": {
                "query_type": query_type,
                "timestamp": RepoRAG._get_timestamp()
            }
        }

        # Extract and format source information
        for doc in response.get("source_documents", []):
            source_info = {
                "content": doc.page_content[:200] + "...",  # Truncate for brevity
                "metadata": doc.metadata
            }
            processed_response["sources"].append(source_info)

        # Create a formatted string representation
        processed_response["formatted_output"] = self._create_formatted_output(processed_response)

        return processed_response

    def _format_for_terminal(self, text: str) -> str:
        """Format text for better terminal readability."""
        # Remove excessive newlines
        text = re.sub(r'\n{3,}', '\n\n', text)

        # Format code blocks
        text = re.sub(
            r'```(\w+)?\n(.*?)\n```',
            lambda m: f"\n{'=' * 40}\n{m.group(2)}\n{'=' * 40}\n",
            text,
            flags=re.DOTALL
        )

        # Format inline code
        text = re.sub(
            r'`([^`]+)`',
            lambda m: f"[{m.group(1)}]",
            text
        )

        return text

    def _create_formatted_output(self, processed_response: Dict[str, Any]) -> str:
        """Create a formatted string representation of the response."""
        sections = []

        # Add answer section
        sections.append("### Answer\n")
        sections.append(processed_response["answer"])

        # Add sources section if available
        if processed_response["sources"]:
            sections.append("\n### Sources\n")
            for idx, source in enumerate(processed_response["sources"], 1):
                source_text = f"\n{idx}. **File:** {source['metadata'].get('file_path', 'unknown')}"
                if 'language' in source['metadata']:
                    source_text += f"\n   **Language:** {source['metadata']['language']}"
                source_text += f"\n   **Preview:** {source['content']}"
                sections.append(source_text)

        # Add metadata section
        sections.append("\n### Metadata\n")
        sections.append(f"**Query Type:** {processed_response['metadata']['query_type']}")
        sections.append(f"**Timestamp:** {processed_response['metadata']['timestamp']}")

        return "\n".join(sections)

    @staticmethod
    def _get_timestamp() -> str:
        """Get current timestamp."""
        from datetime import datetime
        return datetime.now().isoformat()

    def _generate_cache_key(
        self,
        question: str,
        context_filter: Optional[Dict],
        query_type: str
    ) -> str:
        """Generate cache key including context filter and query type."""
        key_content = f"{question}-{str(context_filter)}-{query_type}"
        return hashlib.md5(key_content.encode('utf-8')).hexdigest()

    def _check_cache(self, cache_key: str) -> Optional[Dict]:
        """Check if response is cached."""
        cache_path = os.path.join(self.cache_dir, cache_key)
        if os.path.exists(cache_path):
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        return None

    def _cache_response(self, cache_key: str, response: Dict[str, Any]) -> None:
        """Cache response."""
        cache_path = os.path.join(self.cache_dir, cache_key)
        with open(cache_path, 'wb') as f:
            pickle.dump(response, f)

    def _update_retriever_filter(self, query_type: str, custom_filter: Optional[Dict] = None):
        """Update retriever's filter based on query type and custom filter."""
        base_filter = {}

        # Add query type-specific filters
        if query_type == "code":
            base_filter["content_type"] = "code"
        elif query_type == "documentation":
            base_filter["content_type"] = "markdown"

        # Merge with custom filter if provided
        if custom_filter:
            base_filter.update(custom_filter)

        # Update retriever
        if base_filter:
            self.qa_chain.retriever.search_kwargs["filter"] = base_filter
        else:
            self.qa_chain.retriever.search_kwargs["filter"] = None

    def get_chat_history(self) -> List[Tuple[str, str]]:
        """Get formatted chat history."""
        return [(self.chat_history[i].content, self.chat_history[i + 1].content)
                for i in range(0, len(self.chat_history), 2)]

    def _update_chat_history(self, question: str, answer: str) -> None:
        """Update chat history."""
        self.chat_history.append(HumanMessage(content=question))
        self.chat_history.append(AIMessage(content=answer))

    def clear_memory(self) -> None:
        """Clear chat history."""
        self.chat_history = []
        self.logger.info("Chat history cleared")

    def clear_vector_store(self) -> None:
        """Clear the vector store."""
        if hasattr(self, 'vector_store'):
            self.logger.info("Clearing vector store")
            shutil.rmtree('./repo_chroma_db', ignore_errors=True)
            self.vector_store = None

    def clear_cache(self) -> None:
        """Clear the cache directory."""
        self.logger.info("Clearing cache directory")
        shutil.rmtree(self.cache_dir, ignore_errors=True)
        os.makedirs(self.cache_dir, exist_ok=True)

    def get_stats(self) -> Dict[str, Any]:
        """Get statistics about the RAG system."""
        stats = {
            "total_documents": 0,
            "total_tokens": 0,
            "content_types": {},
            "languages": {},
            "cache_size": 0,
            "chat_history_length": len(self.chat_history) // 2
        }

        # Get vector store stats if available
        if self.vector_store:
            collection = self.vector_store._collection
            docs = collection.get()
            stats["total_documents"] = len(docs["ids"])

            # Analyze metadata
            for metadata in docs["metadatas"]:
                # Count content types
                content_type = metadata.get("content_type", "unknown")
                stats["content_types"][content_type] = stats["content_types"].get(content_type, 0) + 1

                # Count languages
                language = metadata.get("language", "unknown")
                stats["languages"][language] = stats["languages"].get(language, 0) + 1

                # Count tokens
                stats["total_tokens"] += metadata.get("tokens", 0)

        # Get cache size
        cache_size = 0
        for root, _, files in os.walk(self.cache_dir):
            for file in files:
                cache_size += os.path.getsize(os.path.join(root, file))
        stats["cache_size"] = cache_size

        return stats

    def optimize(self) -> None:
        """Optimize the RAG system for better performance."""
        if not self.vector_store:
            return

        try:
            self.logger.info("Starting RAG system optimization")

            # Get current stats
            before_stats = self.get_stats()

            # Optimize vector store
            collection = self.vector_store._collection
            collection.persist()

            # Clear unnecessary cache entries
            self._clean_cache()

            # Get stats after optimization
            after_stats = self.get_stats()

            # Log optimization results
            self.logger.info(f"Optimization completed. Cache size reduced from "
                           f"{before_stats['cache_size']} to {after_stats['cache_size']} bytes")

        except Exception as e:
            self.logger.error(f"Error during optimization: {str(e)}")
            raise

    def _clean_cache(self) -> None:
        """Clean old or invalid cache entries."""
        try:
            # Get all cache files
            cache_files = os.listdir(self.cache_dir)

            # Remove files older than 7 days
            from datetime import datetime, timedelta
            max_age = timedelta(days=7)
            now = datetime.now()

            for cache_file in cache_files:
                cache_path = os.path.join(self.cache_dir, cache_file)
                file_modified = datetime.fromtimestamp(os.path.getmtime(cache_path))

                if now - file_modified > max_age:
                    os.remove(cache_path)

        except Exception as e:
            self.logger.error(f"Error cleaning cache: {str(e)}")
            raise

    def export_config(self) -> Dict[str, Any]:
        """Export current configuration."""
        return {
            "chunk_size": self.chunk_size,
            "chunk_overlap": self.chunk_overlap,
            "k_retrieval": self.k_retrieval,
            "cache_dir": self.cache_dir,
            "stats": self.get_stats()
        }

    @classmethod
    def from_config(cls, config: Dict[str, Any], groq_api_key: str) -> 'RepoRAG':
        """Create a RepoRAG instance from configuration."""
        instance = cls(
            groq_api_key=groq_api_key,
            chunk_size=config.get("chunk_size", 1000),
            chunk_overlap=config.get("chunk_overlap", 200),
            k_retrieval=config.get("k_retrieval", 4),
            cache_dir=config.get("cache_dir", "./cache")
        )
        return instance

# Helper functions for external use
def create_repo_rag(groq_api_key: str, **kwargs) -> RepoRAG:
    """Create a new RepoRAG instance with given configuration."""
    return RepoRAG(groq_api_key=groq_api_key, **kwargs)

def load_repo_rag(config_path: str, groq_api_key: str) -> RepoRAG:
    """Load a RepoRAG instance from a configuration file."""
    with open(config_path, 'r') as f:
        config = pickle.load(f)
    return RepoRAG.from_config(config, groq_api_key)

File: reporag/reporag/structurer.py
Size: 156 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT
Content:
# repoprompter/repoprompter/structurer.py
def structure_text_for_llm(text_content):
    structured_text = "\n".join(text_content)
    return structured_text

File: requirements.txt
Size: 255 bytes
Last Modified: Sat, 04 Jan 2025 14:29:56 GMT
Content:
# Core dependencies
PyGithub>=2.1.1
python-dotenv>=1.0.0

# RAG-specific dependencies
langchain>=0.1.0
langchain-core>=0.1.0
langchain-groq>=0.0.2
langchain-community>=0.0.13
chromadb>=0.4.22
sentence-transformers>=2.2.2

# Text processing
tiktoken>=0.5.2


    Instructions:
    1. Identify the main purpose of the repository.
    2. Summarize the key functionalities provided by the repository.
    3. Highlight any important files or directories and their roles.
    4. Note any dependencies or requirements specified in the repository.
    5. Provide any additional insights or observations about the repository's structure and content.

    Please provide a detailed analysis based on the above instructions.
    