
    The following is the content of a GitHub repository converted into text format. The repository contains various files and directories, each serving a specific purpose. Please analyze and understand the content of this repository.

    Repository Content:
    File: .gitignore
Size: 3139 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/


Directory: .vscode

File: .vscode/extensions.json
Size: 63 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
{
    "recommendations": [
        "github.copilot"
    ]
}

File: .vscode/settings.json
Size: 147 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
{
    "python.testing.pytestArgs": [
        "tests"
    ],
    "python.testing.unittestEnabled": false,
    "python.testing.pytestEnabled": true
}

File: LICENSE
Size: 1069 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
MIT License

Copyright (c) 2024 ahammadnafiz

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


File: README.md
Size: 4187 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:

# FizTorch: A Toy Tensor Library for Machine Learning

![Logo](assets/fiztorch.png)
[![Python Version](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/release/python-3120/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)](https://travis-ci.org/yourusername/FizTorch)


FizTorch is a lightweight deep learning framework designed for educational purposes and small-scale projects. It provides a simple and intuitive API for building and training neural networks, inspired by popular frameworks like PyTorch.

## Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Examples](#examples)
- [Contributing](#contributing)
- [License](#license)

## Features

- **Tensor Operations**: Basic tensor operations with support for automatic differentiation.
- **Neural Network Layers**: Common neural network layers such as Linear and ReLU.
- **Sequential Model**: Easy-to-use sequential model for stacking layers.
- **Functional API**: Functional operations for common neural network functions.

## Installation

To install FizTorch, follow these steps:

1. **Clone the Repository**:
   ```sh
   git clone https://github.com/ahammadnafiz/FizTorch.git
   cd FizTorch
   ```

2. **Set Up a Virtual Environment** (optional but recommended):
   ```sh
   python -m venv fiztorch-env
   source fiztorch-env/bin/activate  # On Windows, use `fiztorch-env\Scripts\activate`
   ```

3. **Install Dependencies**:
   ```sh
   pip install -r requirements.txt
   ```

4. **Install FizTorch**:
   ```sh
   pip install -e .
   ```

## Usage

Here is a simple example of how to use FizTorch to build and train a neural network:

```python
import numpy as np
from fiztorch.tensor import Tensor
from fiztorch.nn import Linear, ReLU, Sequential
import fiztorch.nn.functional as F

# Define a simple neural network
model = Sequential(
    Linear(2, 3),
    ReLU(),
    Linear(3, 1)
)

# Create some input data
input = Tensor([[1.0, 2.0]], requires_grad=True)

# Forward pass
output = model(input)

# Backward pass
output.backward()

# Print the gradients
print(input.grad)
```

## Examples

### MNIST HAND DIGIT TEST
Neural network training on MNIST digits using  FizTorch library with Adam optimizer (configurable learning rate), batch support, real-time accuracy/loss tracking
![Training Process](assets/training_progress.gif)

### California Housing TEST
Neural network training on California Housing Dataset using  FizTorch library
![Training Process](assets/training_progress_regression.gif)


### Linear Layer

```python
from fiztorch.tensor import Tensor
from fiztorch.nn import Linear

# Create a linear layer
layer = Linear(2, 3)

# Create some input data
input = Tensor([[1.0, 2.0]])

# Forward pass
output = layer(input)

# Print the output
print(output)
```

### ReLU Activation

```python
from fiztorch.tensor import Tensor
from fiztorch.nn import ReLU

# Create a ReLU activation
relu = ReLU()

# Create some input data
input = Tensor([-1.0, 0.0, 1.0])

# Forward pass
output = relu(input)

# Print the output
print(output)
```

### Sequential Model

```python
from fiztorch.tensor import Tensor
from fiztorch.nn import Linear, ReLU, Sequential

# Define a sequential model
model = Sequential(
    Linear(2, 3),
    ReLU(),
    Linear(3, 1)
)

# Create some input data
input = Tensor([[1.0, 2.0]])

# Forward pass
output = model(input)

# Print the output
print(output)
```

## Contributing

Contributions are welcome! Please follow these steps to contribute:

1. Fork the repository.
2. Create a new branch (`git checkout -b feature-branch`).
3. Commit your changes (`git commit -am 'Add new feature'`).
4. Push to the branch (`git push origin feature-branch`).
5. Create a new Pull Request.

## License

FizTorch is licensed under the MIT License. See the [LICENSE](https://github.com/ahammadnafiz/FizTorch/blob/main/LICENSE) file for more information.

## Contact

For any questions or feedback, please open an issue or contact the maintainers.

---

Made with ❤️ by [ahammadnafiz](https://github.com/ahammadnafiz)


Directory: assets

File: assets/fiztorch.png (binary or non-UTF-8 content)
Size: 20270 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT

File: assets/time_series_prediction.png (binary or non-UTF-8 content)
Size: 75929 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT

File: assets/training_loss.png (binary or non-UTF-8 content)
Size: 25296 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT

File: assets/training_progress.gif (binary or non-UTF-8 content)
Size: 12917527 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT

File: assets/training_progress_regression.gif (binary or non-UTF-8 content)
Size: 6708542 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT

Directory: examples

File: examples/mnist_example.py
Size: 7659 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from matplotlib.animation import FuncAnimation, PillowWriter
import matplotlib.gridspec as gridspec

from fiztorch.tensor import Tensor
from fiztorch.nn import Linear, ReLU, Sequential
import fiztorch.nn.functional as F
import fiztorch.optim.optimizer as opt

# Original mnist digit data from openml
# def load_mnist_data():
#     try:
#         # Fetch the MNIST dataset from OpenML
#         mnist = fetch_openml('mnist_784', version=1)
#         X, y = mnist.data, mnist.target.astype(int)

#         # Normalize the data
#         scaler = StandardScaler()
#         X = scaler.fit_transform(X)

#         # Split the data into training and testing sets
#         X_train, X_test, y_train, y_test = train_test_split(
#             X, y, test_size=0.2, random_state=42
#         )

#         # Verify data shapes before converting to tensors
#         assert X_train.shape[1] == 784, "Input features should be 784-dimensional"
#         assert len(np.unique(y)) == 10, "Should have 10 classes"

#         return (Tensor(X_train), Tensor(y_train), 
#                 Tensor(X_test), Tensor(y_test))
#     except Exception as e:
#         print(f"Error loading MNIST data: {str(e)}")
#         raise

# def create_model():
#     try:
#         model = Sequential(
#             Linear(784, 128),  # Updated input size to 784
#             ReLU(),
#             Linear(128, 64),
#             ReLU(),
#             Linear(64, 10)
#         )
#         return model
#     except Exception as e:
#         print(f"Error creating model: {str(e)}")
#         raise


def load_mnist_data():
    try:
        digits = load_digits()
        X, y = digits.data, digits.target

        # Normalize the data
        scaler = StandardScaler()
        X = scaler.fit_transform(X)

        # Split the data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # Verify data shapes before converting to tensors
        assert X_train.shape[1] == 64, "Input features should be 64-dimensional"
        assert len(np.unique(y)) == 10, "Should have 10 classes"

        return (Tensor(X_train), Tensor(y_train), 
                Tensor(X_test), Tensor(y_test))
    except Exception as e:
        print(f"Error loading MNIST data: {str(e)}")
        raise

def create_model():
    try:
        model = Sequential(
            Linear(64, 128),
            ReLU(),
            Linear(128, 64),
            ReLU(),
            Linear(64, 10)
        )
        return model
    except Exception as e:
        print(f"Error creating model: {str(e)}")
        raise

def train_epoch(model, optimizer, X_train, y_train, batch_size):
    try:
        indices = np.random.permutation(len(X_train.data))
        total_loss = 0
        n_batches = 0

        for i in range(0, len(X_train.data), batch_size):
            batch_indices = indices[i:i+batch_size]
            X_batch = Tensor(X_train.data[batch_indices])
            y_batch = Tensor(y_train.data[batch_indices])

            optimizer.zero_grad()

            # Forward pass
            logits = model(X_batch)
            loss = F.cross_entropy(logits, y_batch)

            # Backward pass
            loss.backward()
            optimizer.step()

            total_loss += loss.data
            n_batches += 1

        return total_loss / n_batches
    except Exception as e:
        print(f"Error during training epoch: {str(e)}")
        raise

def evaluate(model, X, y):
    try:
        logits = model(X)
        probs = F.softmax(logits, dim=-1)
        predictions = np.argmax(probs.data, axis=1)
        accuracy = np.mean(predictions == y.data)
        return accuracy
    except Exception as e:
        print(f"Error during evaluation: {str(e)}")
        raise

def create_training_animation(train_losses, train_accuracies, test_accuracies, save_path='training_progress.gif'):
    # Create figure and subplots
    fig = plt.figure(figsize=(12, 6))
    gs = gridspec.GridSpec(1, 2, figure=fig)
    ax1 = fig.add_subplot(gs[0, 0])
    ax2 = fig.add_subplot(gs[0, 1])

    # Maximum values for scaling
    max_loss = max(train_losses)
    max_acc = max(max(train_accuracies), max(test_accuracies))

    def animate(frame):
        ax1.clear()
        ax2.clear()

        # Get data up to current frame
        current_losses = train_losses[:frame+1]
        current_train_acc = train_accuracies[:frame+1]
        current_test_acc = test_accuracies[:frame+1]
        epochs = range(1, frame+2)

        # Plot Loss
        ax1.plot(epochs, current_losses, 'b-', label='Train Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('Training Loss over Epochs')
        ax1.legend()
        ax1.set_ylim(0, max_loss * 1.1)
        ax1.grid(True, linestyle='--', alpha=0.7)

        # Plot Accuracy
        ax2.plot(epochs, current_train_acc, 'g-', label='Train Accuracy')
        ax2.plot(epochs, current_test_acc, 'r-', label='Test Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.set_title('Accuracy over Epochs')
        ax2.legend()
        ax2.set_ylim(0, max_acc * 1.1)
        ax2.grid(True, linestyle='--', alpha=0.7)

        plt.tight_layout()

    # Create animation
    n_frames = len(train_losses)
    anim = FuncAnimation(
        fig, 
        animate, 
        frames=n_frames,
        interval=50,  # 50ms between frames
        repeat=True
    )

    # Save as GIF
    writer = PillowWriter(fps=20)
    anim.save(save_path, writer=writer)
    plt.close()

def main():
    try:
        # Load data
        print("Loading MNIST data...")
        X_train, y_train, X_test, y_test = load_mnist_data()

        # Create model and optimizer
        print("Creating model...")
        model = create_model()
        optimizer = opt.Adam(model.parameters(), lr=0.001)

        # Training parameters
        n_epochs = 500
        batch_size = 32

        train_losses = []
        train_accuracies = []
        test_accuracies = []

        # Training loop
        print("Training started...")
        for epoch in range(n_epochs):
            avg_loss = train_epoch(model, optimizer, X_train, y_train, batch_size)
            train_acc = evaluate(model, X_train, y_train)
            test_acc = evaluate(model, X_test, y_test)

            train_losses.append(avg_loss)
            train_accuracies.append(train_acc)
            test_accuracies.append(test_acc)

            if (epoch + 1) % 5 == 0:
                print(f"Epoch {epoch + 1}")
                print(f"Average Loss: {avg_loss:.4f}")
                print(f"Training Accuracy: {train_acc:.4f}")
                print(f"Test Accuracy: {test_acc:.4f}")
                print("-" * 50)

        # Create and save the training animation
        print("Creating training progress animation...")
        # create_training_animation(train_losses, train_accuracies, test_accuracies)
        print("Animation saved as 'training_progress.gif'")

    except Exception as e:
        print(f"Error in main execution: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nTraining interrupted by user")
    except Exception as e:
        print(f"Fatal error: {str(e)}")
        raise

File: examples/regression_example.py
Size: 6354 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pytest
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from fiztorch.tensor import Tensor
from fiztorch.nn import Linear, ReLU, Sequential
from fiztorch.optim.optimizer import SGD
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, PillowWriter

# Store metrics for animation
train_mse_history = []
test_mse_history = []
epochs_history = []

def load_housing_data():
    """Load and preprocess California Housing dataset"""
    try:
        housing = fetch_california_housing()
        X, y = housing.data, housing.target
        
        X_scaler = StandardScaler()
        y_scaler = StandardScaler()
        
        X = X_scaler.fit_transform(X)
        y = y_scaler.fit_transform(y.reshape(-1, 1))
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        return (Tensor(X_train), Tensor(y_train), 
                Tensor(X_test), Tensor(y_test),
                X_scaler, y_scaler)
    except Exception as e:
        print(f"Error loading housing data: {str(e)}")
        raise

def create_model(input_dim):
    """Create a regression model"""
    try:
        model = Sequential(
            Linear(input_dim, 64),
            ReLU(),
            Linear(64, 32),
            ReLU(),
            Linear(32, 1)
        )
        return model
    except Exception as e:
        print(f"Error creating model: {str(e)}")
        raise

def mse_loss(predictions: Tensor, targets: Tensor) -> Tensor:
    """Calculate Mean Squared Error loss"""
    return ((predictions - targets) ** 2).mean()

def train_epoch(model, optimizer, X_train, y_train, batch_size):
    """Train for one epoch"""
    try:
        indices = np.random.permutation(len(X_train.data))
        total_loss = 0
        n_batches = 0
        
        for i in range(0, len(X_train.data), batch_size):
            batch_indices = indices[i:i+batch_size]
            X_batch = Tensor(X_train.data[batch_indices])
            y_batch = Tensor(y_train.data[batch_indices])
            
            optimizer.zero_grad()
            predictions = model(X_batch)
            loss = mse_loss(predictions, y_batch)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.data
            n_batches += 1
        
        return total_loss / n_batches
    except Exception as e:
        print(f"Error during training epoch: {str(e)}")
        raise

def evaluate(model, X, y):
    """Calculate MSE on the dataset"""
    try:
        predictions = model(X)
        mse = mse_loss(predictions, y)
        return mse.data
    except Exception as e:
        print(f"Error during evaluation: {str(e)}")
        raise

def create_training_animation(save_path='training_progress.gif'):
    """Create and save animation of training progress"""
    fig, ax = plt.subplots(figsize=(10, 6))
    
    def animate(frame):
        ax.clear()
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Mean Squared Error')
        ax.set_title('Training Progress')
        
        # Plot data up to current frame
        ax.plot(epochs_history[:frame], train_mse_history[:frame], 
                label='Training MSE', color='blue')
        ax.plot(epochs_history[:frame], test_mse_history[:frame], 
                label='Test MSE', color='red')
        
        ax.legend()
        ax.grid(True)
        
        # Set y-axis limits based on data range
        if len(train_mse_history) > 0:
            max_mse = max(max(train_mse_history), max(test_mse_history))
            min_mse = min(min(train_mse_history), min(test_mse_history))
            ax.set_ylim(min_mse * 0.9, max_mse * 1.1)
    
    # Create animation
    anim = FuncAnimation(
        fig, animate, 
        frames=len(epochs_history), 
        interval=50,  # 50ms between frames
        repeat=False
    )
    
    # Save as GIF
    writer = PillowWriter(fps=20)
    anim.save(save_path, writer=writer)
    plt.close()

def main():
    try:
        # Clear previous history
        train_mse_history.clear()
        test_mse_history.clear()
        epochs_history.clear()
        
        # Load data
        print("Loading California Housing dataset...")
        X_train, y_train, X_test, y_test, _, _ = load_housing_data()
        
        # Create model and optimizer
        print("Creating model...")
        input_dim = X_train.shape[1]
        model = create_model(input_dim)
        optimizer = SGD(model.parameters(), lr=0.01)
        
        # Training parameters
        n_epochs = 200
        batch_size = 32
        
        # Training loop
        print("Training started...")
        best_test_mse = float('inf')
        
        for epoch in range(n_epochs):
            avg_loss = train_epoch(model, optimizer, X_train, y_train, batch_size)
            train_mse = evaluate(model, X_train, y_train)
            test_mse = evaluate(model, X_test, y_test)
            
            # Store metrics for animation
            train_mse_history.append(train_mse)
            test_mse_history.append(test_mse)
            epochs_history.append(epoch)
            
            if test_mse < best_test_mse:
                best_test_mse = test_mse
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch + 1}")
                print(f"Average Loss: {avg_loss:.4f}")
                print(f"Training MSE: {train_mse:.4f}")
                print(f"Test MSE: {test_mse:.4f}")
                print(f"Best Test MSE: {best_test_mse:.4f}")
                print("-" * 50)
        
        # Create and save animation
        print("Creating training animation...")
        create_training_animation()
        print("Animation saved as 'training_progress.gif'")
                
    except Exception as e:
        print(f"Error in main execution: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        pytest.main([__file__])
        main()
    except KeyboardInterrupt:
        print("\nTraining interrupted by user")
    except Exception as e:
        print(f"Fatal error: {str(e)}")
        raise

File: examples/time_series.py
Size: 5408 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
"""
Time Series Prediction Example using FizTorch
Predicts future values of a sine wave with added noise
"""
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))


import numpy as np
from fiztorch import Tensor
from fiztorch.nn import Linear, ReLU, Sequential
import fiztorch.nn.functional as F
from fiztorch.optim import SGD
from fiztorch.utils import DataLoader

import matplotlib.pyplot as plt
from typing import Tuple, List

class TimeSeriesDataset:
    def __init__(self, seq_length: int = 50, num_samples: int = 1000, noise_level: float = 0.1):
        """
        Generate synthetic time series data based on sine waves with noise
        
        Args:
            seq_length: Length of each sequence
            num_samples: Number of sequences to generate
            noise_level: Amount of noise to add to the sine wave
        """
        self.seq_length = seq_length
        t = np.linspace(0, 8*np.pi, num_samples + seq_length)
        
        # Generate sine wave with multiple frequencies
        self.data = (np.sin(t) + 0.5 * np.sin(2*t) + 0.25 * np.sin(3*t))
        
        # Add noise
        self.data += np.random.normal(0, noise_level, self.data.shape)
        
        # Normalize data
        self.data = (self.data - self.data.mean()) / self.data.std()
        
    def get_sequence(self) -> Tuple[np.ndarray, np.ndarray]:
        """Create sequences for training"""
        X, y = [], []
        
        for i in range(len(self.data) - self.seq_length):
            X.append(self.data[i:(i + self.seq_length)])
            y.append(self.data[i + self.seq_length])
            
        return np.array(X), np.array(y)

class TimeSeriesPredictor(Sequential):
    def __init__(self, seq_length: int, hidden_size: int = 64):
        """
        Neural network for time series prediction
        
        Args:
            seq_length: Length of input sequence
            hidden_size: Number of hidden units
        """
        super().__init__(
            Linear(seq_length, hidden_size),
            ReLU(),
            Linear(hidden_size, hidden_size // 2),
            ReLU(),
            Linear(hidden_size // 2, 1)
        )

def train_predictor(model: TimeSeriesPredictor,
                   train_loader: DataLoader,
                   optimizer: SGD,
                   epochs: int = 50) -> List[float]:
    """
    Train the time series predictor
    
    Returns:
        List of training losses
    """
    losses = []
    
    for epoch in range(epochs):
        epoch_loss = 0.0
        batch_count = 0
        
        for X_batch, y_batch in train_loader:
            # Convert to tensors
            X = Tensor(X_batch, requires_grad=True)
            y = Tensor(y_batch.reshape(-1, 1), requires_grad=True)
            
            # Forward pass
            pred = model(X)
            loss = F.mse_loss(pred, y)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.data
            batch_count += 1
        
        avg_loss = epoch_loss / batch_count
        losses.append(avg_loss)
        
        if epoch % 5 == 0:
            print(f"Epoch {epoch}, Loss: {avg_loss:.6f}")
    
    return losses

def visualize_predictions(model: TimeSeriesPredictor,
                        dataset: TimeSeriesDataset,
                        num_predictions: int = 100):
    """Visualize the model's predictions against actual values"""
    X, y = dataset.get_sequence()
    
    # Get predictions for a subset of data
    test_X = X[-num_predictions:]
    test_y = y[-num_predictions:]
    
    # Make predictions
    predictions = []
    for seq in test_X:
        pred = model(Tensor(seq.reshape(1, -1))).data
        predictions.append(pred.item())
    
    # Plot results
    plt.figure(figsize=(12, 6))
    plt.plot(test_y, label='Actual', alpha=0.7)
    plt.plot(predictions, label='Predicted', alpha=0.7)
    plt.legend()
    plt.title('Time Series Prediction')
    plt.xlabel('Time Step')
    plt.ylabel('Value')
    plt.grid(True)
    plt.savefig('time_series_prediction.png')
    plt.close()

def main():
    # Parameters
    seq_length = 50
    batch_size = 32
    hidden_size = 64
    epochs = 500
    
    # Create dataset
    print("Generating dataset...")
    dataset = TimeSeriesDataset(seq_length=seq_length)
    X, y = dataset.get_sequence()
    
    # Create data loader
    data_loader = DataLoader(X, y, batch_size=batch_size, shuffle=True)
    
    # Initialize model and optimizer
    print("Initializing model...")
    model = TimeSeriesPredictor(seq_length, hidden_size)
    optimizer = SGD(model.parameters(), lr=0.01)
    
    # Train model
    print("Training model...")
    losses = train_predictor(model, data_loader, optimizer, epochs)
    
    # Visualize results
    print("Generating visualization...")
    visualize_predictions(model, dataset)
    print("Visualization saved as 'time_series_prediction.png'")
    
    # Plot training loss
    plt.figure(figsize=(10, 5))
    plt.plot(losses)
    plt.title('Training Loss Over Time')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.grid(True)
    plt.savefig('training_loss.png')
    plt.close()
    print("Training loss plot saved as 'training_loss.png'")

if __name__ == "__main__":
    main()

Directory: fiznet

Directory: fiznet/.cph

File: fiznet/.cph/.neural_network.py_29859cdcbdc552440ffcceb4e190e92f.prob
Size: 271 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
{"name":"Local: neural_network","url":"d:\\FizTorch\\fiznet\\neural_network.py","tests":[{"id":1727268627217,"input":"","output":""}],"interactive":false,"memoryLimit":1024,"timeLimit":3000,"srcPath":"d:\\FizTorch\\fiznet\\neural_network.py","group":"local","local":true}

File: fiznet/__init__.py
Size: 341 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
__version__ = '0.1.0'

from .binarymodel import LogisticClassifier
from .neural_network import NN
from .knn import KNNClassifier


__all__ = [
    'LogisticClassifier',
    'NN',
    'KNNClassifier',
]

def get_version():
    return __version__

def get_available_models():
    return [LogisticClassifier, NN, KNNClassifier]

Directory: fiznet/__pycache__

File: fiznet/__pycache__/__init__.cpython-312.pyc (binary or non-UTF-8 content)
Size: 616 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT

File: fiznet/__pycache__/adema_mix.cpython-312.pyc (binary or non-UTF-8 content)
Size: 3285 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT

File: fiznet/__pycache__/base_model.cpython-312.pyc (binary or non-UTF-8 content)
Size: 1278 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT

File: fiznet/__pycache__/binarymodel.cpython-312.pyc (binary or non-UTF-8 content)
Size: 4150 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT

File: fiznet/__pycache__/knn.cpython-312.pyc (binary or non-UTF-8 content)
Size: 2395 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT

File: fiznet/__pycache__/neural_network.cpython-312.pyc (binary or non-UTF-8 content)
Size: 7367 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT

File: fiznet/base_model.py
Size: 758 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
from abc import ABC, abstractmethod
from typing import Any

class BaseModel(ABC):
    @abstractmethod
    def train(self, X: Any, Y: Any, num_iterations: int = None, learning_rate: float = None) -> None:
        """
        Train the model on the given data.

        Args:
            X: Input features
            Y: Target values
            num_iterations: Number of training iterations (optional)
            learning_rate: Learning rate for optimization (optional)
        """
        pass

    @abstractmethod
    def predict(self, X: Any) -> Any:
        """
        Make predictions using the trained model.

        Args:
            X: Input features

        Returns:
            Predictions
        """
        pass


File: fiznet/binarymodel.py
Size: 2174 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import numpy as np
from .base_model import BaseModel

class LogisticClassifier(BaseModel):
    def __init__(self, input_dim, lambda_reg=0.01):
        self.W = np.zeros((input_dim, 1))
        self.b = 0
        self.lambda_reg = lambda_reg  # L2 regularization parameter

    def sigmoid(self, z):
        # More numerically stable sigmoid function
        return np.where(z >= 0, 
                        1 / (1 + np.exp(-z)), 
                        np.exp(z) / (1 + np.exp(z)))

    def forward(self, X):
        z = np.dot(X, self.W) + self.b
        return self.sigmoid(z)

    def compute_cost(self, A, Y):
        m = Y.shape[0]
        cost = -np.sum(Y * np.log(A + 1e-8) + (1 - Y) * np.log(1 - A + 1e-8)) / m
        # Add L2 regularization term
        cost += (self.lambda_reg / (2 * m)) * np.sum(np.square(self.W))
        return cost

    def backward(self, X, A, Y):
        m = X.shape[0]
        dz = A - Y
        dW = np.dot(X.T, dz) / m + (self.lambda_reg / m) * self.W  # Add regularization term
        db = np.sum(dz) / m
        return dW, db

    def update_parameters(self, dW, db, learning_rate):
        self.W -= learning_rate * dW
        self.b -= learning_rate * db

    def train(self, X, Y, num_iterations, learning_rate, early_stopping_rounds=5):
        best_cost = float('inf')
        rounds_without_improvement = 0

        for i in range(num_iterations):
            A = self.forward(X)
            cost = self.compute_cost(A, Y)
            dW, db = self.backward(X, A, Y)
            self.update_parameters(dW, db, learning_rate)

            if i % 100 == 0:
                print(f"Cost after iteration {i}: {cost}")

            # Early stopping
            if cost < best_cost:
                best_cost = cost
                rounds_without_improvement = 0
            else:
                rounds_without_improvement += 1

            if rounds_without_improvement >= early_stopping_rounds:
                print(f"Early stopping at iteration {i}")
                break

    def predict(self, X):
        A = self.forward(X)
        return (A > 0.5).astype(int)

File: fiznet/knn.py
Size: 1016 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import numpy as np
from collections import Counter
from .base_model import BaseModel

class KNNClassifier(BaseModel):
    def __init__(self, k_neighbors=3):
        self.k_neighbors = k_neighbors

    def fit(self, X_train, y_train):
        self.X_train = np.array(X_train)
        self.y_train = np.array(y_train)

    def get_distance(self, a, b):
        return np.linalg.norm(a - b)

    def get_k_neighbors(self, X_test_single):
        distances = [(i, self.get_distance(self.X_train[i], X_test_single)) for i in range(len(self.X_train))]
        distances.sort(key=lambda x: x[1])
        return distances[:self.k_neighbors]

    def predict(self, X_test):
        predictions = []
        for X_test_single in X_test:
            k_neighbors = self.get_k_neighbors(X_test_single)
            k_labels = [self.y_train[i] for i, _ in k_neighbors]
            most_common = Counter(k_labels).most_common(1)[0][0]
            predictions.append(most_common)
        return predictions

File: fiznet/neural_network.py
Size: 4092 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import numpy as np
from .base_model import BaseModel

class NN(BaseModel):
    def __init__(self, layer_dims, lambda_reg=0.0, decay_rate=0.0, keep_prob=1.0):
        self.layer_dims = layer_dims
        self.parameters = self.initialize_parameters()
        self.L = len(layer_dims) - 1
        self.lambda_reg = lambda_reg
        self.decay_rate = decay_rate
        self.keep_prob = keep_prob

    def initialize_parameters(self):
        np.random.seed(1)
        parameters = {}
        for l in range(1, len(self.layer_dims)):
            parameters[f'W{l}'] = np.random.randn(self.layer_dims[l], self.layer_dims[l-1]) * np.sqrt(2. / self.layer_dims[l-1])
            parameters[f'b{l}'] = np.zeros((self.layer_dims[l], 1))
        return parameters

    def relu(self, Z):
        return np.maximum(0, Z)
    
    def relu_derivative(self, Z):
        return Z > 0

    def softmax(self, Z):
        # Subtract the max value from Z for numerical stability
        exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))
        # Compute the softmax values by dividing the exponentials by their sum
        return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)

    def forward_propagation(self, X, is_training=True):
        caches = []
        A = X
        D = None  # Dropout mask
        for l in range(1, self.L + 1):
            A_prev = A
            W = self.parameters[f'W{l}']
            b = self.parameters[f'b{l}']
            Z = np.dot(W, A_prev) + b
            if l == self.L:
                A = self.softmax(Z)
            else:
                A = self.relu(Z)
                if is_training and self.keep_prob < 1.0:
                    D = np.random.rand(A.shape[0], A.shape[1]) < self.keep_prob
                    A *= D
                    A /= self.keep_prob
            cache = (A_prev, W, b, Z, D)
            caches.append(cache)
        return A, caches

    def backward_propagation(self, AL, Y, caches):
        grads = {}
        m = AL.shape[1]
        Y = Y.reshape(AL.shape)
        dZL = AL - Y
        A_prev, WL, bL, ZL, _ = caches[self.L - 1]
        grads[f'dW{self.L}'] = (1/m) * np.dot(dZL, A_prev.T) + (self.lambda_reg / m) * WL
        grads[f'db{self.L}'] = (1/m) * np.sum(dZL, axis=1, keepdims=True)
        dA_prev = np.dot(WL.T, dZL)
        for l in reversed(range(self.L - 1)):
            A_prev, W, b, Z, D = caches[l]
            dZ = dA_prev * self.relu_derivative(Z)
            if self.keep_prob < 1.0:
                dZ *= D
                dZ /= self.keep_prob
            grads[f'dW{l+1}'] = (1/m) * np.dot(dZ, A_prev.T) + (self.lambda_reg / m) * W
            grads[f'db{l+1}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)
            if l > 0:
                dA_prev = np.dot(W.T, dZ)
        return grads

    def update_parameters(self, grads, learning_rate):
        for l in range(1, self.L + 1):
            self.parameters[f'W{l}'] -= learning_rate * grads[f'dW{l}']
            self.parameters[f'b{l}'] -= learning_rate * grads[f'db{l}']

    def train(self, X, Y, num_iterations, learning_rate):
        costs = []
        for i in range(num_iterations):
            AL, caches = self.forward_propagation(X)
            cost = -np.sum(Y * np.log(AL + 1e-8)) / Y.shape[1]
            # Add L2 regularization term to the cost
            l2_cost = 0
            for l in range(1, self.L + 1):
                l2_cost += np.sum(np.square(self.parameters[f'W{l}']))
            cost += (self.lambda_reg / (2 * Y.shape[1])) * l2_cost
            grads = self.backward_propagation(AL, Y, caches)
            self.update_parameters(grads, learning_rate)
            # Decay the learning rate
            learning_rate *= (1. / (1. + self.decay_rate * i))
            if i % 100 == 0:
                costs.append(cost)
                print(f"Cost after iteration {i}: {cost}")
        return costs

    def predict(self, X):
        AL, _ = self.forward_propagation(X, is_training=False)
        return np.argmax(AL, axis=0)

Directory: fiznet/utils

File: fiznet/utils/__init__.py
Size: 252 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
# Import key functions from tools.py
from .tools import accuracy, precision, recall, f1_score

# Define __all__ to specify what gets imported when someone does "from FizNet.utils import *"
__all__ = ['accuracy', 'precision', 'recall', 'f1_score']


File: fiznet/utils/tools.py
Size: 2545 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import numpy as np
from typing import Union, Sequence

def accuracy(y_true: Sequence[Union[int, float]], y_pred: Sequence[Union[int, float]]) -> float:
    """
    Calculate the accuracy of predictions.

    Args:
        y_true (Sequence[Union[int, float]]): True labels.
        y_pred (Sequence[Union[int, float]]): Predicted labels.

    Returns:
        float: Accuracy score.
    """
    return np.mean(np.array(y_true) == np.array(y_pred))

def precision(y_true: Sequence[Union[int, float]], y_pred: Sequence[Union[int, float]], positive_label: Union[int, float] = 1) -> float:
    """
    Calculate the precision score.

    Args:
        y_true (Sequence[Union[int, float]]): True labels.
        y_pred (Sequence[Union[int, float]]): Predicted labels.
        positive_label (Union[int, float]): The label of the positive class. Default is 1.

    Returns:
        float: Precision score.
    """
    true_positives = np.sum((np.array(y_true) == positive_label) & (np.array(y_pred) == positive_label))
    predicted_positives = np.sum(np.array(y_pred) == positive_label)
    return true_positives / predicted_positives if predicted_positives > 0 else 0.0

def recall(y_true: Sequence[Union[int, float]], y_pred: Sequence[Union[int, float]], positive_label: Union[int, float] = 1) -> float:
    """
    Calculate the recall score.

    Args:
        y_true (Sequence[Union[int, float]]): True labels.
        y_pred (Sequence[Union[int, float]]): Predicted labels.
        positive_label (Union[int, float]): The label of the positive class. Default is 1.

    Returns:
        float: Recall score.
    """
    true_positives = np.sum((np.array(y_true) == positive_label) & (np.array(y_pred) == positive_label))
    actual_positives = np.sum(np.array(y_true) == positive_label)
    return true_positives / actual_positives if actual_positives > 0 else 0.0

def f1_score(y_true: Sequence[Union[int, float]], y_pred: Sequence[Union[int, float]], positive_label: Union[int, float] = 1) -> float:
    """
    Calculate the F1 score.

    Args:
        y_true (Sequence[Union[int, float]]): True labels.
        y_pred (Sequence[Union[int, float]]): Predicted labels.
        positive_label (Union[int, float]): The label of the positive class. Default is 1.

    Returns:
        float: F1 score.
    """
    prec = precision(y_true, y_pred, positive_label)
    rec = recall(y_true, y_pred, positive_label)
    return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0

Directory: fiztorch

File: fiztorch/__init__.py
Size: 176 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
# Only import the core Tensor class at root level
from .tensor import Tensor

__version__ = '0.1.0'

# Note: Removed automatic imports of submodules to prevent circular imports

Directory: fiztorch/bin

File: fiztorch/bin/Activate.ps1
Size: 9033 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
<#
.Synopsis
Activate a Python virtual environment for the current PowerShell session.

.Description
Pushes the python executable for a virtual environment to the front of the
$Env:PATH environment variable and sets the prompt to signify that you are
in a Python virtual environment. Makes use of the command line switches as
well as the `pyvenv.cfg` file values present in the virtual environment.

.Parameter VenvDir
Path to the directory that contains the virtual environment to activate. The
default value for this is the parent of the directory that the Activate.ps1
script is located within.

.Parameter Prompt
The prompt prefix to display when this virtual environment is activated. By
default, this prompt is the name of the virtual environment folder (VenvDir)
surrounded by parentheses and followed by a single space (ie. '(.venv) ').

.Example
Activate.ps1
Activates the Python virtual environment that contains the Activate.ps1 script.

.Example
Activate.ps1 -Verbose
Activates the Python virtual environment that contains the Activate.ps1 script,
and shows extra information about the activation as it executes.

.Example
Activate.ps1 -VenvDir C:\Users\MyUser\Common\.venv
Activates the Python virtual environment located in the specified location.

.Example
Activate.ps1 -Prompt "MyPython"
Activates the Python virtual environment that contains the Activate.ps1 script,
and prefixes the current prompt with the specified string (surrounded in
parentheses) while the virtual environment is active.

.Notes
On Windows, it may be required to enable this Activate.ps1 script by setting the
execution policy for the user. You can do this by issuing the following PowerShell
command:

PS C:\> Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

For more information on Execution Policies: 
https://go.microsoft.com/fwlink/?LinkID=135170

#>
Param(
    [Parameter(Mandatory = $false)]
    [String]
    $VenvDir,
    [Parameter(Mandatory = $false)]
    [String]
    $Prompt
)

<# Function declarations --------------------------------------------------- #>

<#
.Synopsis
Remove all shell session elements added by the Activate script, including the
addition of the virtual environment's Python executable from the beginning of
the PATH variable.

.Parameter NonDestructive
If present, do not remove this function from the global namespace for the
session.

#>
function global:deactivate ([switch]$NonDestructive) {
    # Revert to original values

    # The prior prompt:
    if (Test-Path -Path Function:_OLD_VIRTUAL_PROMPT) {
        Copy-Item -Path Function:_OLD_VIRTUAL_PROMPT -Destination Function:prompt
        Remove-Item -Path Function:_OLD_VIRTUAL_PROMPT
    }

    # The prior PYTHONHOME:
    if (Test-Path -Path Env:_OLD_VIRTUAL_PYTHONHOME) {
        Copy-Item -Path Env:_OLD_VIRTUAL_PYTHONHOME -Destination Env:PYTHONHOME
        Remove-Item -Path Env:_OLD_VIRTUAL_PYTHONHOME
    }

    # The prior PATH:
    if (Test-Path -Path Env:_OLD_VIRTUAL_PATH) {
        Copy-Item -Path Env:_OLD_VIRTUAL_PATH -Destination Env:PATH
        Remove-Item -Path Env:_OLD_VIRTUAL_PATH
    }

    # Just remove the VIRTUAL_ENV altogether:
    if (Test-Path -Path Env:VIRTUAL_ENV) {
        Remove-Item -Path env:VIRTUAL_ENV
    }

    # Just remove VIRTUAL_ENV_PROMPT altogether.
    if (Test-Path -Path Env:VIRTUAL_ENV_PROMPT) {
        Remove-Item -Path env:VIRTUAL_ENV_PROMPT
    }

    # Just remove the _PYTHON_VENV_PROMPT_PREFIX altogether:
    if (Get-Variable -Name "_PYTHON_VENV_PROMPT_PREFIX" -ErrorAction SilentlyContinue) {
        Remove-Variable -Name _PYTHON_VENV_PROMPT_PREFIX -Scope Global -Force
    }

    # Leave deactivate function in the global namespace if requested:
    if (-not $NonDestructive) {
        Remove-Item -Path function:deactivate
    }
}

<#
.Description
Get-PyVenvConfig parses the values from the pyvenv.cfg file located in the
given folder, and returns them in a map.

For each line in the pyvenv.cfg file, if that line can be parsed into exactly
two strings separated by `=` (with any amount of whitespace surrounding the =)
then it is considered a `key = value` line. The left hand string is the key,
the right hand is the value.

If the value starts with a `'` or a `"` then the first and last character is
stripped from the value before being captured.

.Parameter ConfigDir
Path to the directory that contains the `pyvenv.cfg` file.
#>
function Get-PyVenvConfig(
    [String]
    $ConfigDir
) {
    Write-Verbose "Given ConfigDir=$ConfigDir, obtain values in pyvenv.cfg"

    # Ensure the file exists, and issue a warning if it doesn't (but still allow the function to continue).
    $pyvenvConfigPath = Join-Path -Resolve -Path $ConfigDir -ChildPath 'pyvenv.cfg' -ErrorAction Continue

    # An empty map will be returned if no config file is found.
    $pyvenvConfig = @{ }

    if ($pyvenvConfigPath) {

        Write-Verbose "File exists, parse `key = value` lines"
        $pyvenvConfigContent = Get-Content -Path $pyvenvConfigPath

        $pyvenvConfigContent | ForEach-Object {
            $keyval = $PSItem -split "\s*=\s*", 2
            if ($keyval[0] -and $keyval[1]) {
                $val = $keyval[1]

                # Remove extraneous quotations around a string value.
                if ("'""".Contains($val.Substring(0, 1))) {
                    $val = $val.Substring(1, $val.Length - 2)
                }

                $pyvenvConfig[$keyval[0]] = $val
                Write-Verbose "Adding Key: '$($keyval[0])'='$val'"
            }
        }
    }
    return $pyvenvConfig
}


<# Begin Activate script --------------------------------------------------- #>

# Determine the containing directory of this script
$VenvExecPath = Split-Path -Parent $MyInvocation.MyCommand.Definition
$VenvExecDir = Get-Item -Path $VenvExecPath

Write-Verbose "Activation script is located in path: '$VenvExecPath'"
Write-Verbose "VenvExecDir Fullname: '$($VenvExecDir.FullName)"
Write-Verbose "VenvExecDir Name: '$($VenvExecDir.Name)"

# Set values required in priority: CmdLine, ConfigFile, Default
# First, get the location of the virtual environment, it might not be
# VenvExecDir if specified on the command line.
if ($VenvDir) {
    Write-Verbose "VenvDir given as parameter, using '$VenvDir' to determine values"
}
else {
    Write-Verbose "VenvDir not given as a parameter, using parent directory name as VenvDir."
    $VenvDir = $VenvExecDir.Parent.FullName.TrimEnd("\\/")
    Write-Verbose "VenvDir=$VenvDir"
}

# Next, read the `pyvenv.cfg` file to determine any required value such
# as `prompt`.
$pyvenvCfg = Get-PyVenvConfig -ConfigDir $VenvDir

# Next, set the prompt from the command line, or the config file, or
# just use the name of the virtual environment folder.
if ($Prompt) {
    Write-Verbose "Prompt specified as argument, using '$Prompt'"
}
else {
    Write-Verbose "Prompt not specified as argument to script, checking pyvenv.cfg value"
    if ($pyvenvCfg -and $pyvenvCfg['prompt']) {
        Write-Verbose "  Setting based on value in pyvenv.cfg='$($pyvenvCfg['prompt'])'"
        $Prompt = $pyvenvCfg['prompt'];
    }
    else {
        Write-Verbose "  Setting prompt based on parent's directory's name. (Is the directory name passed to venv module when creating the virtual environment)"
        Write-Verbose "  Got leaf-name of $VenvDir='$(Split-Path -Path $venvDir -Leaf)'"
        $Prompt = Split-Path -Path $venvDir -Leaf
    }
}

Write-Verbose "Prompt = '$Prompt'"
Write-Verbose "VenvDir='$VenvDir'"

# Deactivate any currently active virtual environment, but leave the
# deactivate function in place.
deactivate -nondestructive

# Now set the environment variable VIRTUAL_ENV, used by many tools to determine
# that there is an activated venv.
$env:VIRTUAL_ENV = $VenvDir

if (-not $Env:VIRTUAL_ENV_DISABLE_PROMPT) {

    Write-Verbose "Setting prompt to '$Prompt'"

    # Set the prompt to include the env name
    # Make sure _OLD_VIRTUAL_PROMPT is global
    function global:_OLD_VIRTUAL_PROMPT { "" }
    Copy-Item -Path function:prompt -Destination function:_OLD_VIRTUAL_PROMPT
    New-Variable -Name _PYTHON_VENV_PROMPT_PREFIX -Description "Python virtual environment prompt prefix" -Scope Global -Option ReadOnly -Visibility Public -Value $Prompt

    function global:prompt {
        Write-Host -NoNewline -ForegroundColor Green "($_PYTHON_VENV_PROMPT_PREFIX) "
        _OLD_VIRTUAL_PROMPT
    }
    $env:VIRTUAL_ENV_PROMPT = $Prompt
}

# Clear PYTHONHOME
if (Test-Path -Path Env:PYTHONHOME) {
    Copy-Item -Path Env:PYTHONHOME -Destination Env:_OLD_VIRTUAL_PYTHONHOME
    Remove-Item -Path Env:PYTHONHOME
}

# Add the venv to the PATH
Copy-Item -Path Env:PATH -Destination Env:_OLD_VIRTUAL_PATH
$Env:PATH = "$VenvExecDir$([System.IO.Path]::PathSeparator)$Env:PATH"


File: fiztorch/bin/activate
Size: 2076 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
# This file must be used with "source bin/activate" *from bash*
# You cannot run it directly

deactivate () {
    # reset old environment variables
    if [ -n "${_OLD_VIRTUAL_PATH:-}" ] ; then
        PATH="${_OLD_VIRTUAL_PATH:-}"
        export PATH
        unset _OLD_VIRTUAL_PATH
    fi
    if [ -n "${_OLD_VIRTUAL_PYTHONHOME:-}" ] ; then
        PYTHONHOME="${_OLD_VIRTUAL_PYTHONHOME:-}"
        export PYTHONHOME
        unset _OLD_VIRTUAL_PYTHONHOME
    fi

    # Call hash to forget past commands. Without forgetting
    # past commands the $PATH changes we made may not be respected
    hash -r 2> /dev/null

    if [ -n "${_OLD_VIRTUAL_PS1:-}" ] ; then
        PS1="${_OLD_VIRTUAL_PS1:-}"
        export PS1
        unset _OLD_VIRTUAL_PS1
    fi

    unset VIRTUAL_ENV
    unset VIRTUAL_ENV_PROMPT
    if [ ! "${1:-}" = "nondestructive" ] ; then
    # Self destruct!
        unset -f deactivate
    fi
}

# unset irrelevant variables
deactivate nondestructive

# on Windows, a path can contain colons and backslashes and has to be converted:
if [ "${OSTYPE:-}" = "cygwin" ] || [ "${OSTYPE:-}" = "msys" ] ; then
    # transform D:\path\to\venv to /d/path/to/venv on MSYS
    # and to /cygdrive/d/path/to/venv on Cygwin
    export VIRTUAL_ENV=$(cygpath /media/nafiz/NewVolume/FizTorch/fiztorch)
else
    # use the path as-is
    export VIRTUAL_ENV=/media/nafiz/NewVolume/FizTorch/fiztorch
fi

_OLD_VIRTUAL_PATH="$PATH"
PATH="$VIRTUAL_ENV/"bin":$PATH"
export PATH

# unset PYTHONHOME if set
# this will fail if PYTHONHOME is set to the empty string (which is bad anyway)
# could use `if (set -u; : $PYTHONHOME) ;` in bash
if [ -n "${PYTHONHOME:-}" ] ; then
    _OLD_VIRTUAL_PYTHONHOME="${PYTHONHOME:-}"
    unset PYTHONHOME
fi

if [ -z "${VIRTUAL_ENV_DISABLE_PROMPT:-}" ] ; then
    _OLD_VIRTUAL_PS1="${PS1:-}"
    PS1='(fiztorch) '"${PS1:-}"
    export PS1
    VIRTUAL_ENV_PROMPT='(fiztorch) '
    export VIRTUAL_ENV_PROMPT
fi

# Call hash to forget past commands. Without forgetting
# past commands the $PATH changes we made may not be respected
hash -r 2> /dev/null


File: fiztorch/bin/activate.csh
Size: 944 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
# This file must be used with "source bin/activate.csh" *from csh*.
# You cannot run it directly.

# Created by Davide Di Blasi <davidedb@gmail.com>.
# Ported to Python 3.3 venv by Andrew Svetlov <andrew.svetlov@gmail.com>

alias deactivate 'test $?_OLD_VIRTUAL_PATH != 0 && setenv PATH "$_OLD_VIRTUAL_PATH" && unset _OLD_VIRTUAL_PATH; rehash; test $?_OLD_VIRTUAL_PROMPT != 0 && set prompt="$_OLD_VIRTUAL_PROMPT" && unset _OLD_VIRTUAL_PROMPT; unsetenv VIRTUAL_ENV; unsetenv VIRTUAL_ENV_PROMPT; test "\!:*" != "nondestructive" && unalias deactivate'

# Unset irrelevant variables.
deactivate nondestructive

setenv VIRTUAL_ENV /media/nafiz/NewVolume/FizTorch/fiztorch

set _OLD_VIRTUAL_PATH="$PATH"
setenv PATH "$VIRTUAL_ENV/"bin":$PATH"


set _OLD_VIRTUAL_PROMPT="$prompt"

if (! "$?VIRTUAL_ENV_DISABLE_PROMPT") then
    set prompt = '(fiztorch) '"$prompt"
    setenv VIRTUAL_ENV_PROMPT '(fiztorch) '
endif

alias pydoc python -m pydoc

rehash


File: fiztorch/bin/activate.fish
Size: 2219 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
# This file must be used with "source <venv>/bin/activate.fish" *from fish*
# (https://fishshell.com/). You cannot run it directly.

function deactivate  -d "Exit virtual environment and return to normal shell environment"
    # reset old environment variables
    if test -n "$_OLD_VIRTUAL_PATH"
        set -gx PATH $_OLD_VIRTUAL_PATH
        set -e _OLD_VIRTUAL_PATH
    end
    if test -n "$_OLD_VIRTUAL_PYTHONHOME"
        set -gx PYTHONHOME $_OLD_VIRTUAL_PYTHONHOME
        set -e _OLD_VIRTUAL_PYTHONHOME
    end

    if test -n "$_OLD_FISH_PROMPT_OVERRIDE"
        set -e _OLD_FISH_PROMPT_OVERRIDE
        # prevents error when using nested fish instances (Issue #93858)
        if functions -q _old_fish_prompt
            functions -e fish_prompt
            functions -c _old_fish_prompt fish_prompt
            functions -e _old_fish_prompt
        end
    end

    set -e VIRTUAL_ENV
    set -e VIRTUAL_ENV_PROMPT
    if test "$argv[1]" != "nondestructive"
        # Self-destruct!
        functions -e deactivate
    end
end

# Unset irrelevant variables.
deactivate nondestructive

set -gx VIRTUAL_ENV /media/nafiz/NewVolume/FizTorch/fiztorch

set -gx _OLD_VIRTUAL_PATH $PATH
set -gx PATH "$VIRTUAL_ENV/"bin $PATH

# Unset PYTHONHOME if set.
if set -q PYTHONHOME
    set -gx _OLD_VIRTUAL_PYTHONHOME $PYTHONHOME
    set -e PYTHONHOME
end

if test -z "$VIRTUAL_ENV_DISABLE_PROMPT"
    # fish uses a function instead of an env var to generate the prompt.

    # Save the current fish_prompt function as the function _old_fish_prompt.
    functions -c fish_prompt _old_fish_prompt

    # With the original prompt function renamed, we can override with our own.
    function fish_prompt
        # Save the return status of the last command.
        set -l old_status $status

        # Output the venv prompt; color taken from the blue of the Python logo.
        printf "%s%s%s" (set_color 4B8BBE) '(fiztorch) ' (set_color normal)

        # Restore the return status of the previous command.
        echo "exit $old_status" | .
        # Output the original/"old" prompt.
        _old_fish_prompt
    end

    set -gx _OLD_FISH_PROMPT_OVERRIDE "$VIRTUAL_ENV"
    set -gx VIRTUAL_ENV_PROMPT '(fiztorch) '
end


File: fiztorch/bin/f2py
Size: 252 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
#!/media/nafiz/NewVolume/FizTorch/fiztorch/bin/python3
# -*- coding: utf-8 -*-
import re
import sys
from numpy.f2py.f2py2e import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())


File: fiztorch/bin/fonttools
Size: 253 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
#!/media/nafiz/NewVolume/FizTorch/fiztorch/bin/python3
# -*- coding: utf-8 -*-
import re
import sys
from fontTools.__main__ import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())


File: fiztorch/bin/imageio_download_bin
Size: 277 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
#!/media/nafiz/NewVolume/FizTorch/fiztorch/bin/python3
# -*- coding: utf-8 -*-
import re
import sys
from imageio.__main__ import download_bin_main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(download_bin_main())


File: fiztorch/bin/imageio_remove_bin
Size: 273 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
#!/media/nafiz/NewVolume/FizTorch/fiztorch/bin/python3
# -*- coding: utf-8 -*-
import re
import sys
from imageio.__main__ import remove_bin_main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(remove_bin_main())


File: fiztorch/bin/numpy-config
Size: 252 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
#!/media/nafiz/NewVolume/FizTorch/fiztorch/bin/python3
# -*- coding: utf-8 -*-
import re
import sys
from numpy._configtool import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())


File: fiztorch/bin/pip
Size: 257 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
#!/media/nafiz/NewVolume/FizTorch/fiztorch/bin/python3
# -*- coding: utf-8 -*-
import re
import sys
from pip._internal.cli.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())


File: fiztorch/bin/pip3
Size: 257 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
#!/media/nafiz/NewVolume/FizTorch/fiztorch/bin/python3
# -*- coding: utf-8 -*-
import re
import sys
from pip._internal.cli.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())


File: fiztorch/bin/pip3.12
Size: 257 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
#!/media/nafiz/NewVolume/FizTorch/fiztorch/bin/python3
# -*- coding: utf-8 -*-
import re
import sys
from pip._internal.cli.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())


File: fiztorch/bin/py.test
Size: 257 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
#!/media/nafiz/NewVolume/FizTorch/fiztorch/bin/python3
# -*- coding: utf-8 -*-
import re
import sys
from pytest import console_main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(console_main())


File: fiztorch/bin/pyftmerge
Size: 250 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
#!/media/nafiz/NewVolume/FizTorch/fiztorch/bin/python3
# -*- coding: utf-8 -*-
import re
import sys
from fontTools.merge import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())


File: fiztorch/bin/pyftsubset
Size: 251 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
#!/media/nafiz/NewVolume/FizTorch/fiztorch/bin/python3
# -*- coding: utf-8 -*-
import re
import sys
from fontTools.subset import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())


File: fiztorch/bin/pytest
Size: 257 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
#!/media/nafiz/NewVolume/FizTorch/fiztorch/bin/python3
# -*- coding: utf-8 -*-
import re
import sys
from pytest import console_main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(console_main())


File: fiztorch/bin/ttx
Size: 248 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
#!/media/nafiz/NewVolume/FizTorch/fiztorch/bin/python3
# -*- coding: utf-8 -*-
import re
import sys
from fontTools.ttx import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())


Directory: fiztorch/nn

File: fiztorch/nn/__init__.py
Size: 324 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
# First import the base module
from .module import Module

# Then import implementations that depend on Module
from .layers import Linear, ReLU
from .sequential import Sequential

# Import independent modules
from . import functional

__all__ = [
    'Module',
    'Linear',
    'ReLU',
    'Sequential',
    'functional',
]

File: fiztorch/nn/functional.py
Size: 6675 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
from typing import Literal
import numpy as np
from abc import ABC, abstractmethod
from ..tensor import Tensor

class Activation(ABC):
    """Abstract base class for activation functions."""
    
    @abstractmethod
    def __call__(self, input: Tensor) -> Tensor:
        """Apply the activation function."""
        pass

class ReLU(Activation):
    """Rectified Linear Unit activation function."""
    
    def __call__(self, input: Tensor) -> Tensor:
        """
        Apply ReLU activation: f(x) = max(0, x)
        
        Args:
            input: Input tensor
            
        Returns:
            Tensor with ReLU activation applied
        """
        return Tensor(np.maximum(0, input.data), requires_grad=input.requires_grad)

class Sigmoid(Activation):
    """Sigmoid activation function."""
    
    def __call__(self, input: Tensor) -> Tensor:
        """
        Apply sigmoid activation: f(x) = 1 / (1 + exp(-x))
        
        Args:
            input: Input tensor
            
        Returns:
            Tensor with sigmoid activation applied
        """
        return Tensor(1 / (1 + np.exp(-input.data)), requires_grad=input.requires_grad)

class Tanh(Activation):
    """Hyperbolic tangent activation function."""
    
    def __call__(self, input: Tensor) -> Tensor:
        """
        Apply tanh activation: f(x) = tanh(x)
        
        Args:
            input: Input tensor
            
        Returns:
            Tensor with tanh activation applied
        """
        return Tensor(np.tanh(input.data), requires_grad=input.requires_grad)

class Softmax(Activation):
    """Softmax activation function."""
    
    def __init__(self, dim: int = -1):
        """
        Initialize Softmax activation.
        
        Args:
            dim: Dimension along which to compute softmax
        """
        self.dim = dim
    
    def __call__(self, input: Tensor) -> Tensor:
        """
        Apply softmax activation with proper gradient computation.
        
        Args:
            input: Input tensor
            
        Returns:
            Tensor with softmax activation applied
        """
        max_val = np.max(input.data, axis=self.dim, keepdims=True)
        exp_x = np.exp(input.data - max_val)
        softmax_output = exp_x / np.sum(exp_x, axis=self.dim, keepdims=True)
        result = Tensor(softmax_output, requires_grad=input.requires_grad)
        
        if input.requires_grad:
            def _backward(gradient: Tensor) -> None:
                s = softmax_output
                grad = s * (gradient.data - np.sum(gradient.data * s, axis=self.dim, keepdims=True))
                input.backward(Tensor(grad, requires_grad=input.requires_grad))
            
            result._grad_fn = _backward
            result.is_leaf = False
        
        return result

class Loss(ABC):
    """Abstract base class for loss functions."""
    
    @abstractmethod
    def __call__(self, input: Tensor, target: Tensor, reduction: str = 'mean') -> Tensor:
        """Compute the loss."""
        pass

class MSELoss(Loss):
    """Mean Squared Error loss function."""
    
    def __call__(self, input: Tensor, target: Tensor, 
                 reduction: Literal['mean', 'sum', 'none'] = 'mean') -> Tensor:
        """
        Compute MSE loss between input and target.
        
        Args:
            input: Predicted values
            target: Target values
            reduction: Type of reduction to apply
            
        Returns:
            Loss tensor
        """
        diff = input - target
        if reduction == 'mean':
            return (diff * diff).sum() / diff.data.size
        elif reduction == 'sum':
            return (diff * diff).sum()
        else:  # 'none'
            return diff * diff

class CrossEntropyLoss(Loss):
    """Cross Entropy loss function with integrated softmax."""
    
    def __call__(self, input: Tensor, target: Tensor,
                 reduction: Literal['mean', 'sum', 'none'] = 'mean') -> Tensor:
        """
        Compute cross entropy loss with integrated softmax.
        
        Args:
            input: Raw logits from the model (batch_size, num_classes)
            target: Class indices (batch_size,)
            reduction: Type of reduction to apply
            
        Returns:
            Loss tensor
        """
        batch_size = len(input.data)
        max_vals = np.max(input.data, axis=-1, keepdims=True)
        exp_x = np.exp(input.data - max_vals)
        softmax_output = exp_x / np.sum(exp_x, axis=-1, keepdims=True)
        log_softmax = np.log(softmax_output + 1e-8)
        
        nll = -log_softmax[np.arange(batch_size), target.data.astype(int)]
        
        if reduction == 'mean':
            loss_value = np.mean(nll)
        elif reduction == 'sum':
            loss_value = np.sum(nll)
        else:  # 'none'
            loss_value = nll
            
        result = Tensor(loss_value, requires_grad=input.requires_grad)
        
        if input.requires_grad:
            def _backward(gradient: Tensor) -> None:
                grad = softmax_output.copy()
                grad[np.arange(batch_size), target.data.astype(int)] -= 1
                
                if reduction == 'mean':
                    grad = grad / batch_size
                
                if not np.isscalar(gradient.data):
                    grad = grad * gradient.data.reshape(-1, 1)
                else:
                    grad = grad * gradient.data
                    
                input.backward(Tensor(grad))
                
            result._grad_fn = _backward
            result.is_leaf = False
        
        return result

# Convenience functions for direct use
def relu(input: Tensor) -> Tensor:
    """Convenience function for ReLU activation."""
    return ReLU()(input)

def sigmoid(input: Tensor) -> Tensor:
    """Convenience function for Sigmoid activation."""
    return Sigmoid()(input)

def tanh(input: Tensor) -> Tensor:
    """Convenience function for Tanh activation."""
    return Tanh()(input)

def softmax(input: Tensor, dim: int = -1) -> Tensor:
    """Convenience function for Softmax activation."""
    return Softmax(dim=dim)(input)

def mse_loss(input: Tensor, target: Tensor, 
             reduction: Literal['mean', 'sum', 'none'] = 'mean') -> Tensor:
    """Convenience function for MSE loss."""
    return MSELoss()(input, target, reduction)

def cross_entropy(input: Tensor, target: Tensor,
                 reduction: Literal['mean', 'sum', 'none'] = 'mean') -> Tensor:
    """Convenience function for Cross Entropy loss."""
    return CrossEntropyLoss()(input, target, reduction)

File: fiztorch/nn/init_functions.py
Size: 1679 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import numpy as np
from ..tensor import Tensor

def uniform_(tensor: Tensor, a: float = 0., b: float = 1.) -> Tensor:
    """Fills the tensor with values drawn from the uniform distribution U(a, b)."""
    tensor.data = np.random.uniform(a, b, size=tensor.shape)
    return tensor

def normal_(tensor: Tensor, mean: float = 0., std: float = 1.) -> Tensor:
    """Fills the tensor with values drawn from the normal distribution N(mean, std²)."""
    tensor.data = np.random.normal(mean, std, size=tensor.shape)
    return tensor

def xavier_uniform_(tensor: Tensor) -> Tensor:
    """Fills the tensor with values using Xavier uniform initialization."""
    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)
    bound = np.sqrt(6. / (fan_in + fan_out))
    return uniform_(tensor, -bound, bound)

def xavier_normal_(tensor: Tensor) -> Tensor:
    """Fills the tensor with values using Xavier normal initialization."""
    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)
    std = np.sqrt(2. / (fan_in + fan_out))
    return normal_(tensor, 0., std)

def zeros_(tensor: Tensor) -> Tensor:
    """Fills the tensor with zeros."""
    tensor.data = np.zeros_like(tensor.data)
    return tensor

def ones_(tensor: Tensor) -> Tensor:
    """Fills the tensor with ones."""
    tensor.data = np.ones_like(tensor.data)
    return tensor

def _calculate_fan_in_and_fan_out(tensor: Tensor) -> tuple:
    """Computes the number of input and output features for a tensor."""
    dimensions = tensor.shape
    if len(dimensions) == 2:  # Linear
        fan_in, fan_out = dimensions
    else:
        raise ValueError("Only 2D tensors supported for now")
    return fan_in, fan_out

File: fiztorch/nn/layers.py
Size: 3530 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import numpy as np
from ..tensor import Tensor
from .module import Module
from .init_functions import xavier_uniform_, zeros_

class Linear(Module):
    """
    A fully connected linear layer.

    Args:
        in_features (int): Size of each input sample.
        out_features (int): Size of each output sample.
        bias (bool): If set to False, the layer will not learn an additive bias. Default: True.
    """
    def __init__(self, in_features: int, out_features: int, bias: bool = True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features

        # Initialize weights using Xavier initialization
        self.weight = Tensor(
            np.empty((out_features, in_features)),
            requires_grad=True
        )
        xavier_uniform_(self.weight)
        self._parameters['weight'] = self.weight

        if bias:
            self.bias = Tensor(
                np.empty(out_features),
                requires_grad=True
            )
            zeros_(self.bias)  # Initialize bias with zeros
            self._parameters['bias'] = self.bias
        else:
            self.bias = None

    def forward(self, x):
        """
        Forward pass for the linear layer.

        Args:
            x (Tensor): Input tensor of shape (N, in_features) where N is the batch size.

        Returns:
            Tensor: Output tensor of shape (N, out_features).
        """
        # Ensure input is a Tensor with requires_grad set
        if not isinstance(x, Tensor):
            x = Tensor(x, requires_grad=True)
            
        if not self.weight.requires_grad:
            self.weight = Tensor(self.weight.data, requires_grad=True)
            
        if self.bias is not None and not self.bias.requires_grad:
            self.bias = Tensor(self.bias.data, requires_grad=True)
        
        # Compute the linear transformation
        out = x @ self.weight.T + self.bias
        
        # Define the backward function for gradient computation
        def _backward(gradient):
            if self.weight.requires_grad:
                weight_grad = gradient.data.T @ x.data
                self.weight.backward(Tensor(weight_grad, requires_grad=True))
            if self.bias is not None and self.bias.requires_grad:
                bias_grad = gradient.data.sum(axis=0)
                self.bias.backward(Tensor(bias_grad, requires_grad=True))
                
        out._grad_fn = _backward
        out.is_leaf = False
        return out

class ReLU(Module):
    """
    Applies the rectified linear unit function element-wise: ReLU(x) = max(0, x)
    """
    def forward(self, x):
        """
        Forward pass for the ReLU activation function.

        Args:
            x (Tensor): Input tensor.

        Returns:
            Tensor: Output tensor where each element is the result of applying ReLU to the corresponding element of the input tensor.
        """
        if not isinstance(x, Tensor):
            x = Tensor(x, requires_grad=True)
            
        # Apply ReLU activation function
        result = Tensor(np.maximum(0, x.data), requires_grad=x.requires_grad)
        
        if x.requires_grad:
            # Define the backward function for gradient computation
            def _backward(gradient):
                grad = gradient.data * (x.data > 0)
                x.backward(Tensor(grad, requires_grad=x.requires_grad))
            result._grad_fn = _backward
            result.is_leaf = False
        return result

File: fiztorch/nn/module.py
Size: 2011 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
from typing import Dict, Iterator, Tuple
from ..tensor import Tensor

class Module:
    """
    Base class for all neural network modules.

    Your models should also subclass this class. Modules can also contain other
    Modules, allowing to nest them in a tree structure. You can assign the submodules
    as regular attributes.

    Attributes:
        _parameters (Dict[str, Tensor]): A dictionary mapping parameter names to Tensors.
        training (bool): Boolean flag indicating whether the module is in training mode.

    Methods:
        __init__():
            Initializes internal Module state, including parameters and training mode.
        
        parameters() -> Iterator[Tensor]:
            Returns an iterator over module parameters.
        
        zero_grad() -> None:
            Sets gradients of all parameters to zero.
        
        __call__(*args, **kwargs):
            Calls the forward method. This makes the module callable.
        
        forward(*args, **kwargs):
            Defines the computation performed at every call. Should be overridden by all subclasses.
        
        train(mode: bool = True) -> 'Module':
            Sets the module in training mode.
        
        eval() -> 'Module':
            Sets the module in evaluation mode.
    """
    def __init__(self):
        self._parameters: Dict[str, Tensor] = {}
        self.training: bool = True

    def parameters(self) -> Iterator[Tensor]:
        for param in self._parameters.values():
            yield param

    def zero_grad(self) -> None:
        for param in self.parameters():
            if param.grad is not None:
                param.grad.data.fill_(0)

    def __call__(self, *args, **kwargs):
        return self.forward(*args, **kwargs)

    def forward(self, *args, **kwargs):
        raise NotImplementedError

    def train(self, mode: bool = True) -> 'Module':
        self.training = mode
        return self

    def eval(self) -> 'Module':
        return self.train(False)

File: fiztorch/nn/sequential.py
Size: 2479 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
from typing import List, Iterator
from .module import Module
from ..tensor import Tensor

class Sequential(Module):
    """
    A sequential container. Modules will be added to it in the order they are passed in the constructor.
    A Sequential module is a container module that can be used to create a neural network by stacking layers sequentially.
    """

    def __init__(self, *layers: Module):
        """
        Initializes the Sequential container with the given layers.

        Args:
            *layers (Module): Variable number of modules to be added to the container.
        """
        super().__init__()
        self.layers = list(layers)
        for idx, layer in enumerate(self.layers):
            self._parameters[f'layer_{idx}'] = layer  # Store each layer in the parameters dictionary

    def forward(self, input: Tensor) -> Tensor:
        """
        Defines the computation performed at every call. Passes the input through each layer in sequence.

        Args:
            input (Tensor): The input tensor to the network.

        Returns:
            Tensor: The output tensor after passing through all layers.
        """
        for layer in self.layers:
            input = layer(input)  # Pass the input through each layer
        return input

    def parameters(self) -> Iterator[Tensor]:
        """
        Returns an iterator over module parameters.

        Yields:
            Iterator[Tensor]: An iterator over the parameters of each layer.
        """
        for layer in self.layers:
            yield from layer.parameters()  # Yield parameters from each layer

    def __getitem__(self, idx: int) -> Module:
        """
        Gets the layer at the given index.

        Args:
            idx (int): Index of the layer to retrieve.

        Returns:
            Module: The layer at the specified index.
        """
        return self.layers[idx]

    def __len__(self) -> int:
        """
        Returns the number of layers in the container.

        Returns:
            int: The number of layers.
        """
        return len(self.layers)

    def append(self, module: Module) -> None:
        """
        Appends a module to the end of the container.

        Args:
            module (Module): The module to append.
        """
        self.layers.append(module)  # Add the module to the list of layers
        self._parameters[f'layer_{len(self.layers)-1}'] = module  # Store the new layer in the parameters dictionary

Directory: fiztorch/optim

File: fiztorch/optim/__init__.py
Size: 45 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
from .optimizer import SGD

__all__ = ['SGD']

File: fiztorch/optim/optimizer.py
Size: 5132 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
from typing import Iterator, List, Optional
from ..tensor import Tensor
import numpy as np

class Optimizer:
    def __init__(self, parameters: Iterator[Tensor]):
        self.parameters = list(parameters)
    
    def zero_grad(self) -> None:
        for param in self.parameters:
            param.grad = None
            
    def step(self) -> None:
        raise NotImplementedError

class SGD(Optimizer):
    def __init__(self, parameters: Iterator[Tensor], lr: float = 0.01, momentum: float = 0.0):
        super().__init__(parameters)
        self.lr = lr
        self.momentum = momentum
        self.velocities: List[Optional[np.ndarray]] = [None for _ in self.parameters]
        
    def step(self) -> None:
        for i, param in enumerate(self.parameters):
            if param.grad is not None:
                if self.momentum > 0:
                    if self.velocities[i] is None:
                        self.velocities[i] = np.zeros_like(param.data)
                    
                    # Update velocity
                    self.velocities[i] = (self.momentum * self.velocities[i] - 
                                        self.lr * param.grad.data)
                    # Update parameters using velocity
                    param.data += self.velocities[i]
                else:
                    # Standard SGD update
                    param.data -= self.lr * param.grad.data

class Adam(Optimizer):
    def __init__(self, parameters: Iterator[Tensor], lr: float = 0.001, 
                 betas: tuple = (0.9, 0.999), eps: float = 1e-8):
        super().__init__(parameters)
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.eps = eps
        self.t = 0  # Time step
        
        # Initialize moment estimates
        self.m: List[Optional[np.ndarray]] = [None for _ in self.parameters]  # First moment
        self.v: List[Optional[np.ndarray]] = [None for _ in self.parameters]  # Second moment
        
    def step(self) -> None:
        self.t += 1
        
        for i, param in enumerate(self.parameters):
            if param.grad is not None:
                grad = param.grad.data
                
                # Initialize moments if empty
                if self.m[i] is None:
                    self.m[i] = np.zeros_like(param.data)
                    self.v[i] = np.zeros_like(param.data)
                
                # Update biased first moment estimate
                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
                
                # Update biased second raw moment estimate
                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * np.square(grad)
                
                # Compute bias-corrected first moment estimate
                m_hat = self.m[i] / (1 - self.beta1 ** self.t)
                
                # Compute bias-corrected second raw moment estimate
                v_hat = self.v[i] / (1 - self.beta2 ** self.t)
                
                # Update parameters
                param.data -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)

class RMSprop(Optimizer):
    def __init__(self, parameters: Iterator[Tensor], lr: float = 0.01, 
                 alpha: float = 0.99, eps: float = 1e-8):
        super().__init__(parameters)
        self.lr = lr
        self.alpha = alpha
        self.eps = eps
        self.square_avg: List[Optional[np.ndarray]] = [None for _ in self.parameters]
        
    def step(self) -> None:
        for i, param in enumerate(self.parameters):
            if param.grad is not None:
                grad = param.grad.data
                
                # Initialize square average if empty
                if self.square_avg[i] is None:
                    self.square_avg[i] = np.zeros_like(param.data)
                
                # Update running average of squared gradients
                self.square_avg[i] = (self.alpha * self.square_avg[i] + 
                                    (1 - self.alpha) * np.square(grad))
                
                # Update parameters
                param.data -= (self.lr * grad / 
                             (np.sqrt(self.square_avg[i]) + self.eps))

class Adagrad(Optimizer):
    def __init__(self, parameters: Iterator[Tensor], lr: float = 0.01, eps: float = 1e-8):
        super().__init__(parameters)
        self.lr = lr
        self.eps = eps
        self.sum_squares: List[Optional[np.ndarray]] = [None for _ in self.parameters]
        
    def step(self) -> None:
        for i, param in enumerate(self.parameters):
            if param.grad is not None:
                grad = param.grad.data
                
                # Initialize sum of squares if empty
                if self.sum_squares[i] is None:
                    self.sum_squares[i] = np.zeros_like(param.data)
                
                # Accumulate squared gradients
                self.sum_squares[i] += np.square(grad)
                
                # Update parameters
                param.data -= (self.lr * grad / 
                             (np.sqrt(self.sum_squares[i]) + self.eps))

File: fiztorch/pyvenv.cfg
Size: 179 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
home = /usr/bin
include-system-site-packages = false
version = 3.12.3
executable = /usr/bin/python3.12
command = /usr/bin/python3 -m venv /media/nafiz/NewVolume/FizTorch/fiztorch


Directory: fiztorch/share

Directory: fiztorch/share/man

Directory: fiztorch/share/man/man1

File: fiztorch/share/man/man1/ttx.1
Size: 5377 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
.Dd May 18, 2004
.\" ttx is not specific to any OS, but contrary to what groff_mdoc(7)
.\" seems to imply, entirely omitting the .Os macro causes 'BSD' to
.\" be used, so I give a zero-width space as its argument.
.Os \&
.\" The "FontTools Manual" argument apparently has no effect in
.\" groff 1.18.1. I think it is a bug in the -mdoc groff package.
.Dt TTX 1 "FontTools Manual"
.Sh NAME
.Nm ttx
.Nd tool for manipulating TrueType and OpenType fonts
.Sh SYNOPSIS
.Nm
.Bk
.Op Ar option ...
.Ek
.Bk
.Ar file ...
.Ek
.Sh DESCRIPTION
.Nm
is a tool for manipulating TrueType and OpenType fonts.  It can convert
TrueType and OpenType fonts to and from an
.Tn XML Ns -based format called
.Tn TTX .
.Tn TTX
files have a
.Ql .ttx
extension.
.Pp
For each
.Ar file
argument it is given,
.Nm
detects whether it is a
.Ql .ttf ,
.Ql .otf
or
.Ql .ttx
file and acts accordingly: if it is a
.Ql .ttf
or
.Ql .otf
file, it generates a
.Ql .ttx
file; if it is a
.Ql .ttx
file, it generates a
.Ql .ttf
or
.Ql .otf
file.
.Pp
By default, every output file is created in the same directory as the
corresponding input file and with the same name except for the
extension, which is substituted appropriately.
.Nm
never overwrites existing files; if necessary, it appends a suffix to
the output file name before the extension, as in
.Pa Arial#1.ttf .
.Ss "General options"
.Bl -tag -width ".Fl t Ar table"
.It Fl h
Display usage information.
.It Fl d Ar dir
Write the output files to directory
.Ar dir
instead of writing every output file to the same directory as the
corresponding input file.
.It Fl o Ar file
Write the output to
.Ar file
instead of writing it to the same directory as the
corresponding input file.
.It Fl v
Be verbose.  Write more messages to the standard output describing what
is being done.
.It Fl a
Allow virtual glyphs ID's on compile or decompile.
.El
.Ss "Dump options"
The following options control the process of dumping font files
(TrueType or OpenType) to
.Tn TTX
files.
.Bl -tag -width ".Fl t Ar table"
.It Fl l
List table information.  Instead of dumping the font to a
.Tn TTX
file, display minimal information about each table.
.It Fl t Ar table
Dump table
.Ar table .
This option may be given multiple times to dump several tables at
once.  When not specified, all tables are dumped.
.It Fl x Ar table
Exclude table
.Ar table
from the list of tables to dump.  This option may be given multiple
times to exclude several tables from the dump.  The
.Fl t
and
.Fl x
options are mutually exclusive.
.It Fl s
Split tables.  Dump each table to a separate
.Tn TTX
file and write (under the name that would have been used for the output
file if the
.Fl s
option had not been given) one small
.Tn TTX
file containing references to the individual table dump files.  This
file can be used as input to
.Nm
as long as the referenced files can be found in the same directory.
.It Fl i
.\" XXX: I suppose OpenType programs (exist and) are also affected.
Don't disassemble TrueType instructions.  When this option is specified,
all TrueType programs (glyph programs, the font program and the
pre-program) are written to the
.Tn TTX
file as hexadecimal data instead of
assembly.  This saves some time and results in smaller
.Tn TTX
files.
.It Fl y Ar n
When decompiling a TrueType Collection (TTC) file,
decompile font number
.Ar n ,
starting from 0.
.El
.Ss "Compilation options"
The following options control the process of compiling
.Tn TTX
files into font files (TrueType or OpenType):
.Bl -tag -width ".Fl t Ar table"
.It Fl m Ar fontfile
Merge the input
.Tn TTX
file
.Ar file
with
.Ar fontfile .
No more than one
.Ar file
argument can be specified when this option is used.
.It Fl b
Don't recalculate glyph bounding boxes.  Use the values in the
.Tn TTX
file as is.
.El
.Sh "THE TTX FILE FORMAT"
You can find some information about the
.Tn TTX
file format in
.Pa documentation.html .
In particular, you will find in that file the list of tables understood by
.Nm
and the relations between TrueType GlyphIDs and the glyph names used in
.Tn TTX
files.
.Sh EXAMPLES
In the following examples, all files are read from and written to the
current directory.  Additionally, the name given for the output file
assumes in every case that it did not exist before
.Nm
was invoked.
.Pp
Dump the TrueType font contained in
.Pa FreeSans.ttf
to
.Pa FreeSans.ttx :
.Pp
.Dl ttx FreeSans.ttf
.Pp
Compile
.Pa MyFont.ttx
into a TrueType or OpenType font file:
.Pp
.Dl ttx MyFont.ttx
.Pp
List the tables in
.Pa FreeSans.ttf
along with some information:
.Pp
.Dl ttx -l FreeSans.ttf
.Pp
Dump the
.Sq cmap
table from
.Pa FreeSans.ttf
to
.Pa FreeSans.ttx :
.Pp
.Dl ttx -t cmap FreeSans.ttf
.Sh NOTES
On MS\-Windows and MacOS,
.Nm
is available as a graphical application to which files can be dropped.
.Sh SEE ALSO
.Pa documentation.html
.Pp
.Xr fontforge 1 ,
.Xr ftinfo 1 ,
.Xr gfontview 1 ,
.Xr xmbdfed 1 ,
.Xr Font::TTF 3pm
.Sh AUTHORS
.Nm
was written by
.An -nosplit
.An "Just van Rossum" Aq just@letterror.com .
.Pp
This manual page was written by
.An "Florent Rougon" Aq f.rougon@free.fr
for the Debian GNU/Linux system based on the existing FontTools
documentation.  It may be freely used, modified and distributed without
restrictions.
.\" For Emacs:
.\" Local Variables:
.\" fill-column: 72
.\" sentence-end: "[.?!][]\"')}]*\\($\\| $\\|   \\|  \\)[   \n]*"
.\" sentence-end-double-space: t
.\" End:

File: fiztorch/tensor.py
Size: 11568 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import numpy as np
from typing import Union, Optional

def _unbroadcast(grad, shape):
    """
    Unbroadcast gradients to match the original tensor shape.

    Parameters:
    grad (np.ndarray): The gradient array to be unbroadcasted.
    shape (tuple): The target shape to unbroadcast to.

    Returns:
    np.ndarray: The unbroadcasted gradient.
    """
    # Handle scalars
    if not shape:
        return np.sum(grad)
        
    # Sum out the broadcasted dimensions
    axes = tuple(range(len(grad.shape) - len(shape)))  # Leading dimensions
    for i, (grad_size, shape_size) in enumerate(zip(grad.shape[len(axes):], shape)):
        if grad_size != shape_size:
            axes += (i + len(axes),)
    if axes:
        return np.sum(grad, axis=axes).reshape(shape)
    return grad

class Tensor:
    def __init__(self, data: Union[np.ndarray, list, float], requires_grad: bool = False):
        """
        Initialize a Tensor object.

        Parameters:
        data (Union[np.ndarray, list, float]): The data to be stored in the tensor.
        requires_grad (bool): If True, gradients will be computed for this tensor.
        """
        if isinstance(data, np.ndarray):
            self.data = data.astype(np.float64)  # Ensure float64 for numerical stability
        else:
            self.data = np.array(data, dtype=np.float64)

        self.requires_grad = requires_grad
        self.grad = None if requires_grad else None
        self._grad_fn = None
        self.is_leaf = True

    @property
    def shape(self):
        """Return the shape of the underlying numpy array"""
        return self.data.shape

    @property
    def T(self):
        """Return a new tensor that is the transpose of this tensor"""
        result = Tensor(self.data.T, requires_grad=self.requires_grad)
        
        if self.requires_grad:
            def _backward(gradient):
                self.backward(Tensor(gradient.data.T))
            result._grad_fn = _backward
            result.is_leaf = False
            
        return result

    def zero_grad(self):
        """Reset the gradient to zero"""
        if self.requires_grad:
            self.grad = None
    
    def backward(self, gradient: Optional[Union['Tensor', np.ndarray]] = None) -> None:
        """
        Compute the gradient of the tensor.

        Parameters:
        gradient (Optional[Union['Tensor', np.ndarray]]): The gradient to be propagated.
        """
        if not self.requires_grad:
            return

        # Handle the case when gradient is None (implicit gradient of 1.0)
        if gradient is None:
            gradient = np.ones_like(self.data)
        elif isinstance(gradient, Tensor):
            gradient = gradient.data

        # Initialize or accumulate the gradient
        if self.grad is None:
            self.grad = Tensor(gradient)
        else:
            self.grad = Tensor(self.grad.data + gradient)  # Create new Tensor instead of modifying data

        # Propagate gradient to inputs if there's a gradient function
        if self._grad_fn is not None:
            self._grad_fn(Tensor(gradient))

    def __add__(self, other: Union['Tensor', float]) -> 'Tensor':
        """
        Add two tensors element-wise.

        Parameters:
        other (Union['Tensor', float]): The tensor or scalar to add.

        Returns:
        Tensor: The result of the addition.
        """
        other_data = other.data if isinstance(other, Tensor) else np.array(other, dtype=np.float64)
        result = Tensor(self.data + other_data, requires_grad=self.requires_grad or 
                    (isinstance(other, Tensor) and other.requires_grad))

        if result.requires_grad:
            def _backward(gradient):
                if self.requires_grad:
                    unbroadcast_grad = _unbroadcast(gradient.data, self.data.shape)
                    self.backward(unbroadcast_grad)
                if isinstance(other, Tensor) and other.requires_grad:
                    unbroadcast_grad = _unbroadcast(gradient.data, other.data.shape)
                    other.backward(unbroadcast_grad)
            result._grad_fn = _backward
            result.is_leaf = False

        return result

    def __mul__(self, other: Union['Tensor', float]) -> 'Tensor':
        """
        Multiply two tensors element-wise.

        Parameters:
        other (Union['Tensor', float]): The tensor or scalar to multiply.

        Returns:
        Tensor: The result of the multiplication.
        """
        other_data = other.data if isinstance(other, Tensor) else np.array(other, dtype=np.float64)
        result = Tensor(self.data * other_data, requires_grad=self.requires_grad or 
                    (isinstance(other, Tensor) and other.requires_grad))

        if result.requires_grad:
            def _backward(gradient):
                if self.requires_grad:
                    grad = gradient.data * other_data
                    unbroadcast_grad = _unbroadcast(grad, self.data.shape)
                    self.backward(Tensor(unbroadcast_grad))
                if isinstance(other, Tensor) and other.requires_grad:
                    grad = gradient.data * self.data
                    unbroadcast_grad = _unbroadcast(grad, other.data.shape)
                    other.backward(Tensor(unbroadcast_grad))
            result._grad_fn = _backward
            result.is_leaf = False

        return result

    def __matmul__(self, other: 'Tensor') -> 'Tensor':
        """
        Perform matrix multiplication between two tensors.

        Parameters:
        other (Tensor): The tensor to multiply with.

        Returns:
        Tensor: The result of the matrix multiplication.
        """
        if not isinstance(other, Tensor):
            raise TypeError("Matrix multiplication is only defined between tensors")
            
        result = Tensor(self.data @ other.data, 
                       requires_grad=(self.requires_grad or other.requires_grad))

        if result.requires_grad:
            def _backward(gradient):
                if self.requires_grad:
                    self.backward(gradient @ other.T)
                if other.requires_grad:
                    other.backward(self.T @ gradient)
            result._grad_fn = _backward
            result.is_leaf = False

        return result

    def __neg__(self) -> 'Tensor':
        """Return the negation of the tensor"""
        return self * -1

    def __sub__(self, other: Union['Tensor', float]) -> 'Tensor':
        """
        Subtract two tensors element-wise.

        Parameters:
        other (Union['Tensor', float]): The tensor or scalar to subtract.

        Returns:
        Tensor: The result of the subtraction.
        """
        return self + (-other if isinstance(other, Tensor) else -np.array(other))

    def __rsub__(self, other: Union['Tensor', float]) -> 'Tensor':
        """
        Subtract this tensor from another tensor or scalar.

        Parameters:
        other (Union['Tensor', float]): The tensor or scalar to subtract from.

        Returns:
        Tensor: The result of the subtraction.
        """
        return (-self) + (other if isinstance(other, Tensor) else np.array(other))

    def __truediv__(self, other: Union['Tensor', float]) -> 'Tensor':
        """
        Divide this tensor by another tensor or scalar element-wise.

        Parameters:
        other (Union['Tensor', float]): The tensor or scalar to divide by.

        Returns:
        Tensor: The result of the division.
        """
        if isinstance(other, (int, float, np.integer)):
            return self * (1.0 / float(other))
        return self * (other ** -1)

    def __pow__(self, power: float) -> 'Tensor':
        """
        Raise the tensor to a power element-wise.

        Parameters:
        power (float): The power to raise the tensor to.

        Returns:
        Tensor: The result of the exponentiation.
        """
        result = Tensor(self.data ** power, requires_grad=self.requires_grad)

        if self.requires_grad:
            def _backward(gradient):
                grad = gradient.data * (power * self.data ** (power - 1))
                self.backward(Tensor(grad, requires_grad=self.requires_grad))
            result._grad_fn = _backward
            result.is_leaf = False

        return result

    def sum(self, axis=None, keepdims=False) -> 'Tensor':
        """
        Compute the sum of the tensor elements over a given axis.

        Parameters:
        axis (Optional[int or tuple of ints]): Axis or axes along which a sum is performed.
        keepdims (bool): If True, the axes which are reduced are left in the result as dimensions with size one.

        Returns:
        Tensor: The result of the summation.
        """
        result = Tensor(np.sum(self.data, axis=axis, keepdims=keepdims), 
                       requires_grad=self.requires_grad)

        if self.requires_grad:
            def _backward(gradient):
                if axis is None:
                    grad = np.full(self.data.shape, gradient.data)
                else:
                    grad = np.expand_dims(gradient.data, axis=axis)
                    grad = np.broadcast_to(grad, self.data.shape)
                self.backward(Tensor(grad))
            result._grad_fn = _backward
            result.is_leaf = False

        return result

    def mean(self, axis=None, keepdims=False) -> 'Tensor':
        """
        Compute the mean of the tensor elements over a given axis.

        Parameters:
        axis (Optional[int or tuple of ints]): Axis or axes along which a mean is performed.
        keepdims (bool): If True, the axes which are reduced are left in the result as dimensions with size one.

        Returns:
        Tensor: The result of the mean computation.
        """
        return self.sum(axis=axis, keepdims=keepdims) / np.prod(np.array(self.data.shape)[axis] if axis is not None else self.data.shape)

    def reshape(self, *shape) -> 'Tensor':
        """
        Reshape the tensor to a new shape.

        Parameters:
        shape (tuple): The new shape.

        Returns:
        Tensor: The reshaped tensor.
        """
        if len(shape) == 1 and isinstance(shape[0], (tuple, list)):
            shape = shape[0]
        
        result = Tensor(self.data.reshape(shape), requires_grad=self.requires_grad)

        if self.requires_grad:
            def _backward(gradient):
                self.backward(Tensor(gradient.data.reshape(self.data.shape), 
                                   requires_grad=self.requires_grad))
            result._grad_fn = _backward
            result.is_leaf = False

        return result
    
    def exp(self) -> 'Tensor':
        """
        Compute the exponential of each element in the tensor.

        Returns:
        Tensor: The result of the exponential computation.
        """
        result = Tensor(np.exp(self.data), requires_grad=self.requires_grad)

        if self.requires_grad:
            def _backward(gradient):
                grad = gradient.data * np.exp(self.data)
                self.backward(Tensor(grad, requires_grad=self.requires_grad))
            result._grad_fn = _backward
            result.is_leaf = False

        return result

    def __repr__(self) -> str:
        """Return a string representation of the tensor"""
        return f"Tensor({self.data}, requires_grad={self.requires_grad})"

    def __str__(self) -> str:
        """Return a string representation of the tensor"""
        return self.__repr__()


Directory: fiztorch/utils

File: fiztorch/utils/__init__.py
Size: 54 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
from .data import DataLoader

__all__ = ['DataLoader']

File: fiztorch/utils/data.py
Size: 848 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import numpy as np
from typing import Iterator, Tuple, List

class DataLoader:
    def __init__(self, data: np.ndarray, labels: np.ndarray, batch_size: int = 32, shuffle: bool = True):
        self.data = data
        self.labels = labels
        self.batch_size = batch_size
        self.shuffle = shuffle

    def __iter__(self) -> Iterator[Tuple[np.ndarray, np.ndarray]]:
        indices = np.arange(len(self.data))
        if self.shuffle:
            np.random.shuffle(indices)

        for start_idx in range(0, len(indices), self.batch_size):
            end_idx = min(start_idx + self.batch_size, len(indices))
            batch_indices = indices[start_idx:end_idx]
            yield self.data[batch_indices], self.labels[batch_indices]

    def __len__(self) -> int:
        return (len(self.data) + self.batch_size - 1) // self.batch_size

File: pytest.ini
Size: 100 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
[pytest]
python_files = test_*.py
python_classes = Test*
python_functions = test_*
testpaths = tests

File: requirements.txt
Size: 32 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
# requirements.txt
numpy>=1.19.0

Directory: tests

File: tests/__init__.py
Size: 79 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
# This file can be empty or contain initialization code for the tests package.


File: tests/check_grads.py
Size: 4362 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


import numpy as np
from typing import List, Tuple

from fiztorch.tensor import Tensor

def test_gradients():
    """Run comprehensive tests for gradient tracking"""
    # Test 1: Simple addition
    def test_addition():
        x = Tensor([2.0], requires_grad=True)
        y = Tensor([3.0], requires_grad=True)
        z = x + y
        z.backward()
        assert np.allclose(x.grad.data, [1.0]), "Addition gradient for x failed"
        assert np.allclose(y.grad.data, [1.0]), "Addition gradient for y failed"
        print("✓ Addition gradient test passed")

    # Test 2: Multiplication
    def test_multiplication():
        x = Tensor([2.0], requires_grad=True)
        y = Tensor([3.0], requires_grad=True)
        z = x * y
        z.backward()
        assert np.allclose(x.grad.data, [3.0]), "Multiplication gradient for x failed"
        assert np.allclose(y.grad.data, [2.0]), "Multiplication gradient for y failed"
        print("✓ Multiplication gradient test passed")

    # Test 3: Matrix multiplication
    def test_matmul():
        x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
        y = Tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)
        z = x @ y
        z.backward(Tensor(np.ones_like(z.data)))
        expected_x_grad = np.array([[11., 15.], [11., 15.]])
        expected_y_grad = np.array([[4., 4.], [6., 6.]])
        assert np.allclose(x.grad.data, expected_x_grad), "Matrix multiplication gradient for x failed"
        assert np.allclose(y.grad.data, expected_y_grad), "Matrix multiplication gradient for y failed"
        print("✓ Matrix multiplication gradient test passed")

    # Test 4: Broadcasting
    def test_broadcasting():
        x = Tensor([[1.0], [2.0]], requires_grad=True)
        y = Tensor([[3.0, 4.0]], requires_grad=True)
        z = x + y  # Broadcasting
        z.backward(Tensor(np.ones_like(z.data)))
        assert np.allclose(x.grad.data, [[2.0], [2.0]]), "Broadcasting gradient for x failed"
        assert np.allclose(y.grad.data, [[2.0, 2.0]]), "Broadcasting gradient for y failed"
        print("✓ Broadcasting gradient test passed")

    # Test 5: Power operation
    def test_power():
        x = Tensor([2.0], requires_grad=True)
        z = x ** 2
        z.backward()
        assert np.allclose(x.grad.data, [4.0]), "Power gradient failed"
        print("✓ Power operation gradient test passed")

    # Test 6: Complex computation graph
    def test_complex_graph():
        x = Tensor([1.0], requires_grad=True)
        y = Tensor([2.0], requires_grad=True)
        a = x * y
        b = a + y
        c = b ** 2
        c.backward()
        # dc/dx = 2(xy + y) * y
        # dc/dy = 2(xy + y) * (x + 1)
        expected_x_grad = 2 * (1 * 2 + 2) * 2
        expected_y_grad = 2 * (1 * 2 + 2) * (1 + 1)
        assert np.allclose(x.grad.data, [expected_x_grad]), "Complex graph gradient for x failed"
        assert np.allclose(y.grad.data, [expected_y_grad]), "Complex graph gradient for y failed"
        print("✓ Complex computation graph test passed")

    # Test 7: Sum operation
    def test_sum():
        x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
        y = x.sum()
        y.backward()
        assert np.allclose(x.grad.data, np.ones_like(x.data)), "Sum gradient failed"
        print("✓ Sum operation gradient test passed")

    # Test 8: Mean operation
    def test_mean():
        x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
        y = x.mean()
        y.backward()
        assert np.allclose(x.grad.data, np.ones_like(x.data) * 0.25), "Mean gradient failed"
        print("✓ Mean operation gradient test passed")

    # Run all tests
    tests = [
        test_addition,
        test_multiplication,
        test_matmul,
        test_broadcasting,
        test_power,
        test_complex_graph,
        test_sum,
        test_mean
    ]

    print("Starting gradient tests...\n")
    for test in tests:
        try:
            test()
        except AssertionError as e:
            print(f"❌ {test.__name__} failed: {str(e)}")
        except Exception as e:
            print(f"❌ {test.__name__} failed with unexpected error: {str(e)}")
    print("\nGradient tests completed.")

# Run the tests
test_gradients()

File: tests/final_tests.py
Size: 3917 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pytest
import numpy as np
from fiztorch import Tensor
from fiztorch.optim.optimizer import SGD

def test_tensor_creation():
    # Test different data types
    data_types = [
        [[1, 2], [3, 4]],  # list of lists
        np.array([[1, 2], [3, 4]]),  # numpy array
        2.5,  # scalar
        [1, 2, 3]  # 1D list
    ]
    
    for data in data_types:
        t = Tensor(data)
        assert isinstance(t.data, np.ndarray)
        if isinstance(data, (list, np.ndarray)):
            assert np.array_equal(t.data, np.array(data))
        else:
            assert t.data == data

def test_tensor_properties():
    t = Tensor([[1, 2, 3], [4, 5, 6]])
    assert t.shape == (2, 3)
    assert np.array_equal(t.T.data, np.array([[1, 4], [2, 5], [3, 6]]))
    assert t.requires_grad == False
    
    t_grad = Tensor([[1, 2], [3, 4]], requires_grad=True)
    assert t_grad.requires_grad == True
    assert t_grad.is_leaf == True

def test_tensor_arithmetic():
    x = Tensor([[1, 2], [3, 4]], requires_grad=True)
    y = Tensor([[5, 6], [7, 8]], requires_grad=True)
    
    # Addition
    z = x + y
    assert np.array_equal(z.data, np.array([[6, 8], [10, 12]]))
    
    # Subtraction
    z = x - y
    assert np.array_equal(z.data, np.array([[-4, -4], [-4, -4]]))
    
    # Multiplication
    z = x * y
    assert np.array_equal(z.data, np.array([[5, 12], [21, 32]]))
    
    # Division
    z = x / 2
    assert np.array_equal(z.data, np.array([[0.5, 1], [1.5, 2]]))

def test_matmul():
    x = Tensor([[1, 2], [3, 4]], requires_grad=True)
    y = Tensor([[5, 6], [7, 8]], requires_grad=True)
    z = x @ y
    assert np.array_equal(z.data, np.array([[19, 22], [43, 50]]))

def test_broadcasting():
    # Broadcasting scalar
    x = Tensor([[1, 2], [3, 4]], requires_grad=True)
    y = Tensor(2, requires_grad=True)
    z = x + y
    assert np.array_equal(z.data, np.array([[3, 4], [5, 6]]))
    
    # Broadcasting vectors
    x = Tensor([[1], [2]], requires_grad=True)
    y = Tensor([[3, 4]], requires_grad=True)
    z = x + y
    assert np.array_equal(z.data, np.array([[4, 5], [5, 6]]))

def test_broadcasting_gradients():
    # Test gradient computation with broadcasting
    x = Tensor([[1], [2]], requires_grad=True)  # 2x1 matrix
    y = Tensor([[3, 4, 5]], requires_grad=True)  # 1x3 matrix
    z = x + y  # Broadcasting to 2x3
    z.backward()
    
    # Check shapes
    assert x.grad.shape == (2, 1)
    assert y.grad.shape == (1, 3)
    
    # Check values
    assert np.array_equal(x.grad.data, np.array([[3], [3]]))  # Sum across broadcast dimension
    assert np.array_equal(y.grad.data, np.array([[2, 2, 2]]))  # Sum across broadcast dimension

def test_complex_operations():
    x = Tensor([[1, 2], [3, 4]], requires_grad=True)
    y = Tensor([[5, 6], [7, 8]], requires_grad=True)
    
    # Complex computation: f = (x + y) * (x * y)
    z = (x + y) * (x * y)
    z.backward()
    
    assert x.grad is not None
    assert y.grad is not None

def test_reshape():
    x = Tensor([[1, 2, 3], [4, 5, 6]], requires_grad=True)
    y = x.reshape(3, 2)
    assert y.shape == (3, 2)
    y.backward()
    assert x.grad.shape == (2, 3)

def test_sum_and_mean():
    x = Tensor([[1, 2], [3, 4]], requires_grad=True)
    
    # Test sum
    y = x.sum()
    assert y.data == 10
    y.backward()
    assert np.array_equal(x.grad.data, np.ones_like(x.data))
    
    # Test mean
    x.grad = None  # Reset gradient
    y = x.mean()
    assert y.data == 2.5
    y.backward()
    assert np.array_equal(x.grad.data, np.ones_like(x.data) * 0.25)

def test_power():
    x = Tensor([2, 3], requires_grad=True)
    y = x ** 2
    y.backward()
    assert np.array_equal(y.data, np.array([4, 9]))
    assert np.array_equal(x.grad.data, np.array([4, 6]))

if __name__ == "__main__":
    pytest.main([__file__])

File: tests/test_basic.py
Size: 507 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pytest
import numpy as np
from fiztorch import Tensor

def test_tensor_creation():
    data = [[1, 2], [3, 4]]
    t = Tensor(data)
    assert np.array_equal(t.data, np.array(data))

def test_tensor_addition():
    t1 = Tensor([1, 2, 3])
    t2 = Tensor([4, 5, 6])
    result = t1 + t2
    assert np.array_equal(result.data, np.array([5, 7, 9]))

if __name__ == "__main__":
    pytest.main([__file__])

File: tests/test_imports.py
Size: 857 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import pytest

def test_all_imports():
    # Import core
    from fiztorch import Tensor
    
    # Import nn modules
    from fiztorch.nn import Linear, ReLU, Sequential
    import fiztorch.nn.functional as F
    
    # Import optim
    from fiztorch.optim import SGD
    
    # Import utils
    from fiztorch.utils import DataLoader
    
    # Basic tensor test
    t = Tensor([1, 2, 3])
    assert t.data.tolist() == [1, 2, 3]
    
    # Basic nn test
    model = Sequential(
        Linear(2, 3),
        ReLU()
    )
    
    # Basic optimizer test
    optimizer = SGD(model.parameters(), lr=0.01)
    
    # Basic dataloader test
    import numpy as np
    loader = DataLoader(np.array([[1, 2], [3, 4]]), np.array([0, 1]), batch_size=1)
    
    assert True  # If we got here, all imports worked

if __name__ == "__main__":
    pytest.main([__file__])

File: tests/test_nn.py
Size: 2418 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pytest
import numpy as np
from fiztorch.tensor import Tensor
from fiztorch.nn import Linear, ReLU, Sequential
import fiztorch.nn.functional as F

def test_linear_layer():
    # Standard test
    layer = Linear(2, 3)
    input = Tensor([[1.0, 2.0]])
    output = layer(input)
    assert output.shape == (1, 3), f"Expected shape (1, 3), got {output.shape}"

    # Edge case: Empty tensor
    input = Tensor([])
    with pytest.raises(ValueError):
        layer(input)

    # Edge case: Incorrect input shape
    input = Tensor([1.0])  # Not a 2D tensor
    with pytest.raises(ValueError):
        layer(input)

def test_relu():
    relu = ReLU()

    # Standard test
    input = Tensor([-1.0, 0.0, 1.0])
    output = relu(input)
    assert np.array_equal(output.data, np.array([0.0, 0.0, 1.0])), \
        f"ReLU output mismatch: {output.data}"

    # Edge case: Large positive and negative values
    input = Tensor([-1e6, 1e6])
    output = relu(input)
    assert np.array_equal(output.data, np.array([0.0, 1e6])), \
        f"ReLU failed with large values: {output.data}"

def test_sequential():
    model = Sequential(
        Linear(2, 3),
        ReLU(),
        Linear(3, 1)
    )
    input = Tensor([[1.0, 2.0]])
    output = model(input)
    assert output.shape == (1, 1), f"Expected shape (1, 1), got {output.shape}"

    # Edge case: Empty input
    input = Tensor([])
    with pytest.raises(ValueError):
        model(input)

def test_functional():
    input = Tensor([-1.0, 0.0, 1.0])

    # Test ReLU
    output = F.relu(input)
    assert np.array_equal(output.data, np.array([0.0, 0.0, 1.0])), \
        f"Functional ReLU output mismatch: {output.data}"

    # Test sigmoid
    output = F.sigmoid(input)
    expected = 1 / (1 + np.exp(-input.data))
    assert np.allclose(output.data, expected, atol=1e-6), \
        f"Sigmoid output mismatch: {output.data} vs {expected}"

    # Test softmax
    output = F.softmax(input)
    expected = np.exp(input.data) / np.sum(np.exp(input.data))
    assert np.allclose(output.data, expected, atol=1e-6), \
        f"Softmax output mismatch: {output.data} vs {expected}"
    assert np.isclose(np.sum(output.data), 1.0, atol=1e-6), \
        f"Softmax probabilities do not sum to 1: {np.sum(output.data)}"

if __name__ == "__main__":
    pytest.main([__file__])

File: tests/test_required_grad.py
Size: 856 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
from fiztorch.tensor import Tensor
from fiztorch.tensor import Tensor
from fiztorch.nn import Linear, ReLU, Sequential
import fiztorch.nn.functional as F
from fiztorch.optim.optimizer import SGD

# Initialize model parameters
W = Tensor(np.random.randn(2, 3), requires_grad=True)
b = Tensor(np.zeros(3), requires_grad=True)

# Dummy dataset
X = Tensor(np.random.randn(5, 2))  # Input
y = Tensor(np.array([0, 1, 2, 1, 0]))  # Target

# Training loop
optimizer = SGD([W, b], lr=0.1)
for epoch in range(50):
    # Forward pass
    logits = X @ W + b
    loss = F.cross_entropy(logits, y)

    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Log loss
    print(f"Epoch {epoch+1}, Loss: {loss.data}")


File: tests/test_tensor.py
Size: 1280 bytes
Last Modified: Tue, 31 Dec 2024 20:01:33 GMT
Content:
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pytest
import numpy as np
from fiztorch.tensor import Tensor

def test_tensor_creation():
    # Test various ways of creating tensors
    data = [[1, 2], [3, 4]]
    t1 = Tensor(data)
    assert np.array_equal(t1.data, np.array(data))
    
    t2 = Tensor(5.0)
    assert t2.data == 5.0
    
    t3 = Tensor(np.array([1, 2, 3]))
    assert np.array_equal(t3.data, np.array([1, 2, 3]))

def test_tensor_operations():
    t1 = Tensor([1, 2, 3], requires_grad=True)
    t2 = Tensor([4, 5, 6], requires_grad=True)
    
    # Test addition
    result = t1 + t2
    assert np.array_equal(result.data, np.array([5, 7, 9]))
    
    # Test multiplication
    result = t1 * t2
    assert np.array_equal(result.data, np.array([4, 10, 18]))

def test_backward_propagation():
    # Test simple backward pass
    x = Tensor(2.0, requires_grad=True)
    y = x * x
    y.backward()
    assert x.grad.data == 4.0
    
    # Test more complex backward pass
    x = Tensor(2.0, requires_grad=True)
    y = Tensor(3.0, requires_grad=True)
    z = x * y + y
    z.backward()
    assert x.grad.data == 3.0
    assert y.grad.data == 3.0
    
if __name__ == "__main__":
    pytest.main([__file__])


    Instructions:
    1. Identify the main purpose of the repository.
    2. Summarize the key functionalities provided by the repository.
    3. Highlight any important files or directories and their roles.
    4. Note any dependencies or requirements specified in the repository.
    5. Provide any additional insights or observations about the repository's structure and content.

    Please provide a detailed analysis based on the above instructions.
    